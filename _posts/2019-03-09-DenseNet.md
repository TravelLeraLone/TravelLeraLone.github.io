---
layout: post
title: Densely Connected Convolutional Networks
tags: Notes CNN Classification NetStruc
categories: Paper
excerpt: 本文考虑到每次向下一层传递的过程中，并不是所有信息都是新学习到的，于是采用了一种很简单但也很巧妙的方式来简化每次向下传递时的操作。这样的思路和处理非常有趣，可以在之后的工作中借鉴
---

**原文链接：[Densely Connected Convolutional Networks (2018)](https://arxiv.org/pdf/1608.06993.pdf)**

DenseNet，每一层和每一个后续的层以一种feed-forward fashion相连

优点：解决梯度消失问题，加强特征的propagation和reuse，充分的减少了参数个数

# Introduction

通过concatenation而不是summation

一大优点是减少了参数个数，因为可以大幅度减少冗余的特征图计算。实验发现（ResNet相似）大量layers其实起到很小的作用，且每个layer除了计算它产生的特征图外，还需要传递下去从上一层来的需要preserve的特征，于是这样做可以大量减少这样的计算，从而使每一层需要的conv filter个数减少，从而减少参数

# DenseNets

![ds]({{ "/ds.png" | prepend: site.imgrepo }})

其中H_l被定义为一个组合函数，包含三个连续的操作：BN，ReLU，3x3Conv

用transition layer（也接受之前所有的层的输出作为输入）连接不同的Dense Block，本文使用的transition layer为：BN+1x1Conv+2x2average pooling

Growth rate（k）：每一层的输出层数，本文使用12，可以很小因为会接纳前面的所用层的输出作为输入，仅代表没层增加的channel个数，所以叫成长率

但是由于没层可以有很高channel数的输入，所以在3x3Conv前加一系列1x1Conv作为Bottleneck layer被证实特别有用，故而H_l最终为BN-ReLU-Conv(1x1)-BN-ReLU-Conv(3x3)，本文中让1x1Conv输出4k的channel数

继续压缩（compression）以提高性能，让transition层进行压缩，如果其接受了m个channel，instead of m-channel output，本文让其生成theta*m下取整的channel数的输出，theta在0和1之间，本文本文使用0.5
