---
layout: post
title: CS231n学习笔记
tags: Notes CNN ML
categories: Study
excerpt: 转行起始阶段的学习资料，CS231n的详细笔记，多是卷积神经网络的基础知识，但后面的部分和实际研究中使用的东西已经很接近了，多是某一个特定领域的前沿研究中最基础的部分，在进入某个领域的时候还是很有翻阅的必要的。
---

# History & Brief introduction

1. Animal vision development <br>
2. The invention of cameras <br>
3. Hubel & Wiesel, 1959: How mammals' vision processing mechanism like <br>
4. CV start <br>
<br>
5. David Marr: proposing a mechanism <br>
    - Primal Sketch: edges, bars, ends, virtual lines, etc. <br>
    - 2.5-D Sketch: piece togather the surfaces, depth, etc. <br>
    - 3-D Model Representation <br>
6. Generalized Cylinder & Pictorial Structure: <br>
    - Every matter can be compsed of simple geomatric configuration <br>
7. David Lowe:  <br>
    - Using lines and edges to represent <br>
<br>
8. Vision too hard --- Start from image segmentation <br>
9. Specified & using computer learning --- Real-time face detect <br>
10. Feature based object recognation: 'SIFT' <br>
11. Histogram of Gradients & Deformable Part Model <br>
<br>
12. PASCAL Visual Object Challenge --- Test the prograss we have made in object recognation <br>
13. Overfit problem <br>
14. ImageNet; ImageNet Large Scale Visual Recognition Challenge: whether the first five label contain the correct one <br>
15. 2012 huge dorp of error rate --- Using of CNN <br>
16. Other challenges: Object detaction, Action classification, Image caption, etc. <br>
<br>
17. The first appear of origin CNN --- 1998 <br>
18. Why it is only popular recent years <br>
    - Moore's law & Increasing computation <br>
    - Data explosion <br>
<br>
19. The other challenge beyond just object recognation: <br>
    - Movement recognation <br>
    - 3-D 重建 <br>
    - Recognation of every single pixel of the image <br>
    - Understand the story behind the image, which means truly understanding picture <br>
<br>
<br>

# Image classification pipeline

1. The problem: Semantic gap: <br>
The huge gap between the semantic idea of a cat(or sth.) and the huge matrix of pixel value that the computer is actually seeing.  <br>
2. Challenges: <br>
    - View point <br>
    - Illumination <br>
    - Deformable <br>
    - Occlusion <br>
    - Background clutter <br>
    - Intraclass variation <br>
3. Attempts: find edges, corners, etc.<br>
Very hard & having little scalable <br>
<br>
4. **Some Point of View:**
    - origin pictures
    - images as points in a high dementional space

## K&N An example of simple classification method
5. Data-driven approach: Split the API into two functions, one train, one predict <br>
    - Collect a dataset of images and labels <br>
    - Use Machine Learning to train a classifier <br>
    - Evaluata the classifier on new images <br>
6. Exercise: Nearest Neighbors(CIFAR10 dataset) <br>
    - Train step: memrize all picture <br>

    - Predict step: compare and find the most similar one to the test picture <br>

    - What is 'similar'? L1(Manhattan) distance:  <br>
        
        $$
        d=\sum{abs(a_{ij}-b_{ij})}
        $$
        

        ``` py
        import numpy as py
        
        class NearestNeighbor:
            def __init__(self):
                pass
        
            def train(self, X, y):
                # X is N x D where each row is an example. y is a 1-D of size N contain labels
                self.X_train = X
                self.y_train = y
            
            def predict(self, X):
                num_test = X.shape[0]
                Y_predict = np.zeros(num_test, dtype= self.y_train.dtype) 
        
                for i in range(num_test):
                    distances = np.csum(np.abs(self.X_train - X[i, :]), axis = 1)
                    min_index = np.argmin(distances)
                    Y_predict = self.y_train[min_index]
        
                return Y_predict
        ```

    - Problem: Train O(1), Predict O(N). But we want is slow training and fast predicting <br>
7. K-Nearest Neighbors: (some small improvements of 5.) <br>
    - taking a mojority vote from K closest points <br>

    - Using L2(Euclidean) distance instead: <br>
        <br> $$
        d=\sqrt{\sum{(a_{ij}-b_{ij})^2}}
        $$
        

        - BTW: L1 is dependent upon the choice of coordination system. So somehow it may be more fit if the individual entry of your vector has specific meaning for your task <br>
        - BTW: By just changing the distance matrix, it can be applied to multiple tasks <br>
        <br>
8. Hyperparameters: (like the K value and distance matix in the example above) Very problem dependent and almost not able to learn from huge data. Therefore, most people just try each of them out. <br>
    - Idea 1: choose ones work best on training data.  <br>
        Terrible(example above, K = 1 always do perfect on training data), never do it. <br>
    - Idea 2: Split the training data to train and test part. Choose ones work best on test date <br>
        Terrible, never do it. Because no idea how algorithm will peform on new data. <br>
    - Idea 3: Split the training data into 3 parts, train, validation, test. Train on the train set, choose on validation data, then try algorithm on test set only once at last, to test whether it is good. <br>
        Better <br>
    - Idea 4: Cross_validation. Split data into folds and test try each fold as validation and average the results(accuracy), then do the same as Idea 3 <br>
        Even better, but not use often in prectice<br>

## Linear classification: Another example

9. Parametric Approach: input image x, weight parameter W, so we compose a function f(x, W), to get 10 numbers giving class scors(as above, using CIFAR10, so 10 categories in total) <br>
10. Linear Classifier: $f(x, W) = W \times x + b$ <br>
    - CIFAR10 x 32 x 32 x 3 reshape to 3072 x 1 <br>
    - W 10 x 3072 <br>
    - b 10 x 1. To understand b, if the training data has more cats than dogs, then it reasonable to predict that b_cat is larger than b_dog <br>
    - Can be considered as a template matching method. Each row of W is a template of the category <br>
    - Restrain: only one template for a class, once a huge differece come out, it try to average all the picture to get the only template <br>
11. Hard cases for a linear classifier: **Consider f as a line in high-D space** <br>
    - pairity problem: 一三象限、二四象限; such as odd or even people of animals <br>
    - multimodel: island in other classes <br>
12. Score function: $s = f(x; W) = Wx$ <br>

<br>
<br>

# Loss Function and Optimization

1. Todo from last time: <br>
    - Def a loss function that quantifies our unhappiness with the scores across the training data <br>
    - Come up with a way of efficiently finding the parameters that minimize the loss function --- optimization <br>

## Loss Function

2. Def: tell how good our current classifier is. <br>
<br> $$
L = \frac{1}{N} \times \sum{L_i(f(x_i, W), y_i)}
$$
   <br>

3. Ways of understanding:
    - To set a loss func, what you actually do is to tell the computer what kinds of errors you care about more and what kinds of errors you can kind of accept --- the linear and square func is an example <br>
    <br>

4. Example 1 :Multi-class SVM(Support Vector Machine) loss: (Hinge loss) <br>
    - Fomula: 
      <br> $$
      L_i = \sum_{j\neq y_i} {max(0, s_j - s_{y_i} + \Delta)}
      $$
       (where Delta is a sort of safety margin we set) <br>

    - Min 0; Max infinity <br>

    - Trick: At the beginning, we randomly choose W, so all s should ~= to 0, and all approximatly equal, so there should be $L_i \approx ClassNum - 1$ <br>

    - If we square the value in sum, we'll get a different loss func, and hopefully get a different W --- A trick we can use when figuring out our own loss func <br>

    - If we get a zero loss W, W is definatly not unique --- 2W is also zero loss <br>
      <br>

5. **That's because we tell the classifier to find the W that fits the training data. But in practice we dont really care about it, we really care about the performance of the classifier on test data !!!** <br>

6. Regularization: to encourage the model to choose a "sinple" W --- To solve the problem:<br>
    - <br> $$]
        L = \frac{1}{N}\times(\sum_i{L_i(f(x_i, W), y_i)}) + \lambda\times R(W)
        $$

         <br>

    - **L = Data loss + lambda * Regularization loss** <br>

    - lamda is a very important Hyperparameter <br>

    - Examples: L2, L1, Elastic(beta*L2+L1) <br>
        - Important difference: consider what as complexity <br>
        - L2 prefer to spread the influence to all pixels; While L1 prefer to concentrate <br>
        <br>

7. Example 2: Softmax loss(Multinomial Logistic Regression)
    - Softmax function: normalized probabilities of the classes <br>
      <br> $$
      P(Y = k | X = x_i) = e ^ {s_k} / \sum_j e ^ {s_j}
      $$
       <br>

    - Softmax loss func: The true class's probabilities should close to 1 <br>
      <br> $$
      L_i = - \log(P(Y = y_i | X = x_i))
      $$
       <br>

    - Min 0; Max infinity <br>
      But to get 0, we need infinit number at the correct class and minus infinit number at others. So we'll never get zero loss <br>

    - Trick: when all s ~= 0, and all approximatly equal, so there should be $L_i \approx - \log(1/ClassNum) = \log(ClassNum)$ <br>
    <br>

8. Contrast the two loss func: <br>
    - How we choose to interpret the scores to measure the badness <br>
    - What they want to do: <br>
        - SVM just want the correct class's scors much bigger than others; <br>
        - Softmax want it to go towards infinity and others go towards minus infinity <br>

## Optimization

9. Strategy: Follow the slope --- Gradient, pointing to the greatest increasing direction <br>
10. Numerical & Analytic: to debug & to train <br>
11. Gradient Descent:<br>
    - Step size (also called Learning rate): the single most important hyperparameter! <br>
    - Update rules <br>
    - Stochastic Gradient Descent(SGD): Because N is super large, instead using all N inputs, we use a minibatch of samples of inputs<br>
<br>
12. Image Features and The Motivation: In prectice, we prefer not to fit the raw pixels, but the "features" we "get" from them <br>
    - Example 1: Color Histogram <br>
    - Example 2: Histogram of Oriented Gradients(HoG, Edges direction and distribution) <br>
    - Example 3: Bag of Words(Define visual words) <br>

<br>
<br>

# Backpropagation and Neural Networks

## Backpropagation

1. Computational graph: 类似流程图，每个框代表origin variable or 运算 <br>
2. A simple example: `f(x,y,z) = (x + y) * z` <br>
    - Draw the graph: <br>
        ```
        x ---v (q) 
        y -> + ----v (f)
        z -------> * -->
        ```

    - Assign every intermediate variable a name: <br>
      <br> $$
      q = x + y (\frac{\partial q}{\partial x} = 1;\frac{\partial q}{\partial y} = 1)
      $$
       <br>
      <br> $$
      f = q \times z (\frac{\partial f}{\partial q}= z; \frac{\partial f}{\partial z} = q)
      $$
       <br>

    - To get Gradients of f: we start at the end and go back, and each step calc 
      <br> $$
      \frac{\partial f}{\partial v}
      $$
       (v is a intermediate or origin variable) (using Chain rule) <br>
      <br>
3. Backpropagation: <br>
    - Local node: <br>
    Some inputs `x, y` , a simple calc `f` , some outputs `z` <br>
    It just aware of the immediate surrounding, which is the things above <br>
    - Local gradient: <br>
    For a local node we can get the gradient between the inputs and the output such as `pdz / pdx; pdz / pdy` <br>
    Use them to multiply the gradient get from behind the node(which is `pdfinal / pdz` called Upstream gradient), we can get the gradient between the final output and the inputs of this node <br>
    If there are more than one outputs, which is more than one upstream gradients, add them up as the total upstream input of this node <br>
    - From the end of the computational graph go back the the beginning, we can get the full gradient <br>
4. We can group some simple calc `f` togather to get a little coplex, as long as we can directly write down the local gradient <br>
    - Such as sigmoid function: 
      <br> $$
      \sigma(x) = 1 / (1 + e^{-x})
      $$
       <br>
      <br> $$
      \frac {d\sigma(x)}{dx} = (1 - \sigma(x)) \times \sigma(x)
      $$
       <br>

    - Can be viewed as a trade-off between numerical calc and computational calc <br>
5. Patterns in backward flow: <br>
    - Add gate: gradient distributor; <br>
    - Max gate: gradient router; <br>
    - Mul(multiply) gate: gradient switcher <br>
<br>
6. Gradients for vectorized code (which means that x, y, z are vectors instead of numbers): <br>
    - All the local gradients and the upstream gradients are Jacobian Matrixes. Basically just using the chain rule to calc <br>
    - Can calc elementwise or write it out in vectorized (and matrix) multiply form
    - If the final output is a number, not vector, the full gradient will have the same shape as its corresponding input <br>
    - output是vector情况？ 对Matrix 的梯度为三维矩阵，但Gradient Descent 是Loss Function 的梯度，不是Hypothesis， 输出是number， 不会是Vector <br>

## Neural Networks

7. First see it as a function, without the brain staff: <br>
    - Before: Linear score function: `f = W * x` <br>
    - Now: 2-layer Neural Network: `f = W_2 * max(0, W_1 * x)` <br>
    - **The non-linearity that the max function brings is very important and necessery** <br>
    - Understand from picture view: <br>
    W_1 is a collection of many templates, and W_2 assign the tamplate to classes <br>
    - We can make it even deeper by repeating "add non-linearity and the another layer of linear regression" <br>
8. Inspiration from neural system <br>
9. Non-linearity: called activation function <br>
Such as `Sigmoid, Leaky ReLU, tanh, Maxout, ReLU, ELU` <br>
10. The Architectures: "3-layer Neural Network" or "2-hidden_layer Neural Nerwork" <br>

<br>
<br>

# Convoltional Neural Networks (ConvNet)

1. A bit of history <br>

## From a function perspective (with out brain stuff)

2. Recall: Fully Connected Layer <br>
    <br>

3. Convolution Layer <br>
    - Instead of stretching to a 1-D vector, we keep the structure of the image --- Preserve spatial structure <br>
    - Convolve: the filter with the image i.e. "slide over the image spatially computing dot products" <br>
    Note that the filters always extend the full deoth of the input volume <br>
    - To get the dot product, we stretching the filter into a 1-D vector `w`, and stretching the overlapping chunk of the image into 1-D vector `x`, and then get a number output by `w^T * x + b` <br>
    Or consider as overlap and then element-wise multiplication and add up <br>
    - To do the "slide", basically, center the filter on top of every pixel in this input volume, do the dot product and get a number to fill in the corresponding position in the output matrix (2-D for a 3-D input) (called activation maps) <br>
    - Apply several filters (can be same shape) to the same input, then we can get the same number of activation maps out <br>
    - ConvNet is a sequence of Convolutional Layers, interspersed with activation functions <br>

4. To understand the ConvNet, along the depth of ConvNet (usually the depth of the matrix in the layer is also deeper), the Layers can be considered as looking for featrues from simple to complex <br>
    Such as from local feature to lines then to corners and blobs to the resemble of blobs <br>

5. To understand the filter, we can also see it as a template. By visualizing the filter, we can find out what parttern the filter is looking for, that is: what kind of input will get the larger output <br>
    <br>

6. Stride: the step size that the filter slide on the input matrix <br>

7. <br> $$
    Output\_size=(N_{input} - N_{filter}) / Stride + 1`
    $$
   <br>
  <br>

8. Zero pad: To make it possible to set the filter's center at the corners of the input, we do zero pad <br>
  With the Stride of 1, we can then maintain the size of the output, preventing it from shrinking <br>
  <br>

9. 1 x 1 convolution layers make perfect sense, they mix the information along depth <br>

## The brain view of convolution layer

10. Filter also is called receptive field <br>

## Other layers in the ConvNet

11. Pooling layers (Downsampling) <br>
    - makes the representations smaller and more manageable <br>
    - operates over each activation map independently <br>
    - Only spacially, do nothing to the depth <br>
    - Example: Max Pooling <br>
    - Also instead of doing pooling, we can also use stride to do the same thing --- downsampling <br>
12. ReLU or Other non-linearities: the same as Neural Networks <br>
13. Fully Connected Layer (FC layer) <br>

<br>
<br>

# Train Neural Networks

## Activation Functions

1. Sigmoid: 
    <br> $$
    \sigma(x) = 1 / (1 + e^{-x})
    $$
     <br>
    - Squashes numbers to range [0, 1] <br>
    - Historically popular since they have nice interpretation as a saturating "firing rate" of a neuron <br>
    - Three problems: <br>
        - Saturated neurons "kill" the gradients: <br>
        When inputs are very far away from 0, then the gradient become 0, and that is bad in the procedure in the backprop <br>
        - Output are not zero centered: <br>
        That means the gradients on w will always all positive or negative, that would make the descenting less efficient **???** <br>
        - exp() is a bit compute expensive (less important) <br>

2.  <br> $$
    \tanh(x) = (e^x - e^{-x}) / (e^x + e^{-x})
    $$
     <br>

    - Squashes numbers to range [-1, 1] <br>
    - Zero centered <br>
    - Still, saturated neurons "kill" the gradients <br>

3. Rectified Linear Unit (ReLU): 
    <br> $$
    f(x) = max(0, x)
    $$
     <br>
    - Does not saturate (in + region) <br>
    - Very computationally efficient <br>
    - Converges much faster than sigmoid/tanh in practice <br>
    - Actually more biologically plausible than sigmoid <br>
    - Problems: <br>
        - Not zero-centered output <br>
        - Saturate in the - region: <br>
        When you have a bad initializing weight matrix, or you have a learning rate too large <br>
        To reduce the possibility of dead ReLU, people like to initialize it with slightly positive biases (e.g. 0.01)

4. Leaky ReLU: 
    <br> $$
    f(x) = max(0.01x, x)
    $$
     <br>
    - Does not saturate (in + region) <br>
    - Very computationally efficient <br>
    - Converges much faster than sigmoid/tanh in practice <br>
    - Will not "die" <br>

5. PReLU: 
    <br> $$
    f(x) = max(\alpha * x, x)
    $$
     with \alpha as a parameter that can learn by compute <br>

6. Exponential Linear Units (ELU):
    <br> $$
    f(x) = x>0 ? x : \alpha * (e^x - 1)
    $$
    <br>

    - All benefits of ReLU <br>
    - Closer to zero mean outputs <br>
    - Negative saturation regime compared with Leaky ReLU adds some robustness to noise <br>
    - But computation of exp() <br>

7. Maxout "Neuron": 
    <br> $$
    max(w_1^T \times x + b_1, w_2^T \times x + b_2)
    $$
     <br>
    - Will not saturate, will not die <br>
    - Double the parametar number <br>

## Data Preprocessing

8. Zero-mean & Normalize <br>
    - Subtract the mean image (e.g. AlexNet) or the per-channel mean (e.g. VGGNet) <br>
    Important because if it's not zero-centered then loss function will be too sensitive to small perturbations <br>
    - Not really do normalize in picture processing <br>
9. PCA & Whitening (more complex, usually dont do in picture processing) <br>
<br>
10. When we do the data preprocessing to the training set, we need to do the exact same calc with the exact same value to the test set <br>

## Weight initialization

11. First idea: Small random numbers <br>
    - `w = 0.01 * np.random.randn(D, H)` <br>
    - Do good in small network, but have problem when it get deeper <br>
    picture **???**
12. Xavier initialization: `W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)` <br>
    - Do good when use tanh <br>
    - To fit ReLU ('cause it kill half of the inputs), one possible modification is `W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in / 2)` <br>
13. Still a research area <br>
MSRA another way you can use <br>

## Batch Normalization

14. Idea: want to keep activation in a gaussian range <br>
15. To make each dimension of a batch of actications at some layer unit gaussian, apply: `xk = (xk - E[xk]) / sqrt_root(Var[xk])` <br>
16. Usually inserted after FC layers (each demention) or Convolutional layers (jointly) and before non-linearity (apply for each neuron) <br>
17. Usually then allow the network to squash the range if it wants to `yk = gamak * xk + betak`. Notice that the net work can learn `gamak = sqrt_root(Var[xk]); betak = E[xk]` to recover the identity mapping <br>
18. Benefit: <br>
    - Improves gradient flow through the network <br>
    - Allows higher learning rates <br>
    - Kind of more robust <br>
    - Reduces the strong dependence on initialization <br>
    - Act as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe <br>
19. Note that: At test time the BatchNorm layer functions differently. The mean/std not computed based on the batch. Instead, a single fixed empirical mean of activations during training is used. (e.g. can be estimated during training with running averages) <br>

## Babysitting the Learning Process

20. The starting steps when apply ConvNet:
    - Step 1: Preprocess the data <br>
    - Step 2: Choose the architecture <br>
    Notable: Double check the loss is reasonable (The trick learned in loss function, and also loss should go up when enable the regularization) <br>
    - Step 3: Try to train <br>
        - Tip: First Make sure that you can overfit very small portion of the training data very well (with very small loss and accuracy 1.00) <br>
        - Start with small regularizatioin and find learning rate that makes the loss go down (not go down: too low; exploding: too high) <br>
### **TODO**

## Hyperparameter Optimization

21. Cross-validation Strategy (in stages) <br>
    - First stage: only a few epochs to get rough idea of waht parameters work <br>
    - Second stage: longer runinig time, finer serch <br>
    - Repeat as necessary <br>
    <br>
    - Tips for what doesnt work: If the cost is ever > 3 * original cost. Then we can break out early <br>
    - Note it's best to optimize in log space ( `10**uniform(s,e)` ) <br>
    - And compare to Grid Search, Random Search is actually better. Because we can get a better sense of what the function looks like <br>
22. Some special situation: <br>
    - loss flat for some time then go down: maybe bad initializing <br>
    - big gap between train and validation accuracy: maybe overfitting <br>
    - no gap between them: consider increase capacity <br>
23. Track the ratio of weight updates / weight magnitudes: around 0.001 or so is good <br>

## Fancier Optimization

24. Problem with SGD: <br>
    - If the loss function changes quickly in one direction and slowly in another (has high condition number, which is the ratio of largest to smallest singular value of the Hessian matrix) <br>
    Then the SGD would get a very slow progress along shallow dimension, and we would get a zig-zag jitter along steep direction <br>
    In high-D this would happen more often <br>
    - May stack in a local minima or a saddle point <br>
    In high-D, the problem is more about saddle points and less about local minima, and also not exactly at the saddle point but near the sadlle point <br>
    - Out gradients come from minibatches so they can be noisy <br>
    <br>

25. Minor improvement: SGD + Momentum <br>
    - Build up "velocity" as a running mean of gradients and go the direction of velocity <br>

    - `rho` gives the "friction" of the velocity contribute to the next velocity; Typically rho = 0.9 or 0.99 <br>

    - The fomula: 
      <br> $$
      v_{t+1} = \rho \times v_t + SGD \\ x_{t+1} = x_t - \alpha \times v_{t+1}
      $$
       <br>

    - Solve all three problems! <br>

26. Nesterov Momentum: <br>
    Instead of adding the gradient from the point we now standing on, we start from the point now go as the velocity before updating and calc the gradient there, then go back to the point, and calc the new velocity <br>
    <br> $$
    v_{t+1} = \rho \times v_t - \alpha \times \Delta f(x_t + \rho \times v_t) \\ x_{t+1} = x_t + v_{t+1}
    $$
     <br>
    Another form 
    <br> $$
    v_{t+1} = \rho \times v_t - \alpha \times \Delta f(x_t) \\ x_{t+1} = x_t - \rho \times v_t + (1 + \rho) \times v_{t+1} = x_t + v_{t+1} + \rho \times (v_{t+1} - v_t)
    $$
     <br>
    <br>

27. AdaGrad: <br>
    - Keep adding the sqr of these SGDs and 
      <br> $$
      x -= \alpha \times \frac {SGD} {\sqrt{grad\_squared} + (1e-7)}
      $$
       <br>

    - As the training goes, grad_squared become smaller, so the actually learning rate become smaller --- good for convex, otherwise, may still get stuck at saddle point <br>

28. Improve: RMSProp <br>
    - Instead of `grad_squared += SGD * SGD`, we do `grad_squared = decay_rate * grad_squared + (1 - decay_rate) * SGD * SGD` where `decay_rate` is commonly 0.9 or 0.99 <br>
    <br>

29. Adam algorithm <br>
    - <br> $$
      first\_moment = \beta_1 \times first\_moment + (1 - \beta_1) \times SGD\\
      second\_moment = \beta_2 \times second\_moment + (1 - beta2) * SGD * SGD\\
      x -= learning\_rate \times first\_moment / (\sqrt{second\_moment} + (1e-7))
      $$

       <br>

    - But if we initialize the `second_moment = 0` and because the `beta2` is 0.9 or 0.99 typically, whcih is near to 1. That is to say, for the early steps, we'll always have small second_moment, then there will be a hugh step as we divide it. And that's a problem <br>

    - To solve it: add a bias term (t means the current time step)<br>
      <br> $$
      first\_unbias = first\_moment / (1 - \beta_1^t)\\
      second\_unbias = second\_moment / (1 - \beta_2^t)\\
      x -= learning\_rate \times first\_unbias / (\sqrt{second\_unbias} + (1e-7))
      $$
      <br>
      <br>

30. Problem for all these: axis dependent. That is to say, if we rotate the picture, the algorithm cannot really unrotate it <br>
    <br>

31. Learning rate decay: strt with a relatively large learning rate and then decay it along the training <br>
    - Step decay: decay learning rate by a factor every few epochs <br>
      exponential decay: 
      <br> $$
      \alpha = \alpha_0 \times e^{-k \times t}
      $$
       <br>
      1/t decay: 
      <br> $$
      \alpha = \alpha_0 / (1 + k \times t)
      $$
       <br>

    - Common with SGDMomentum, less common with Adam <br>

    - Second order hyperparemeter <br>
    <br>

32. All the algorithm is First-Order Optimization <br>
    Second-Order Optimization: Newton's Method But H^-1 too big, so there's improvement to approximate Hessian <br>
    (But they dont actully handle the stochastic case very nicely, and tend not to work very well with non-convex problems) <br>
    - Quasi-Newton methods
    - L-BFGS

## Regularization

33. All the above is about how to reduce our training error, but we dont really care about that, what we really care about is the reducing the gap between the training set and unsee samples (test set) <br>
<br>
34. Model Ensembles: Tend to improve by 2% <br>
    - Trian multiple independent models <br>
    - At test time average their results <br>
    - Tricks: <br>
        - Instead of training independent models, use multiple snapshots of a single model during training <br>
        - Polyak averaging: Instead of using parameter vectors, use the average of it along the training ??? <br>
<br>
25. Regularizations: To improve single-model performance <br>
36. Dropout: In each forward pass, randomly set some neurons to zero, and every time we do a forward pass, we'll set a different set of the neurons (randomly) to zero <br>
    - Probablity of dropout is a hyperparameter; 0.5 is common <br>
    - Often do it on FC layers; <br>
    Sometimes use it also on convolutional layers; <br>
    And on convolutional layers sometimes instead of droping neurons, we might drop entire feature maps randomly <br>
    - Understand why this works <br>
        - Help prevent co-adaptation of features, which is to say prevent it from distribute the decision to too many "overlaping" features <br>
        - Can also view as training a large ensemble of models <br>
    - At test time, to wipe out randomness <br>
        - Consider a single neuron which inputs are `x, y` and output is `a` <br>

        - During training time we have: 
          <br> $$
          E[a] = (1-Prob)^2 \times (w_1 \times x + w_2 \times y) + Prob\times(1-Prob) \times (w_1 \times x + 0) + Prob\times(1-Prob) \times (0 + w_2 \times y) + Prob^2 \times (0 + 0) = Prob \times (w_1 \times x + w_2 \times y)
          $$
           <br>

        - So at test time, we just run the model without dropout and then multiply the output by dopout probablity <br>

        - Inverted dropout: Insread multiply Prob at test time, we divide Prob during training time (to do test more efficiently) <br>
        <br>
37. Common pattern of Regularization: <br>
    - Training: add some kind of randomness <br>
    - Testing: average out randomness (or approximatly) <br>
38. Another examples: <br>
    - Batch Normalization <br>
    - Data Augmentation: Train on the randomly transformations of the origin pictures (flips, crops, scales, color jitter, rotating, shearing, etc.) **Consider how can we transform the data without changing the label of it** <br>
    - DropConnect <br>
    - Fractional Max Pooling <br>
    - Stochastic Depth <br>

## Transfer Learing

39. Another point of view to overfit: you dont have enough data <br>
40. Step of Transfer Learning: <br>
    - Train on a Large enough and similar Dataset <br>
    - Freeze the hidden layers, and reinitialize and train the last FC layers on the small dataset you want to aplly on <br>
    - When you have a larger dataset, then maybe consider updating larger parts of the network <br>
    - Note that you probably want to drop the learning rate when you do the transfer <br>
41. Pretrain and Finetuning is very commonly use in many areas <br>

<br>
<br>

# Deep Learning Hardware and Software

## CPU vs. GPU

1. GPU: (DL prefer NVIDIA) <br>
    - larger number of cores (more than 100 times), but each core is much slower (small clock speed) and "dumber" <br>
    - have their own pretty large RAMs and own caching system <br>
    - Great for parallel tasks with pretty high similarity <br>
2. Example: Matrix Multiplication <br>
3. Programming GPUs <br>
    - CUDA (NVIDIA only) <br>
    - OpenCL (runs on anything, but usually slower) <br>
4. Data Transfer problem (from disk to GPUs). Solutions: <br>
    - Read all data into RAM (if your data is relatively small) <br>
    - Usd SSD instead of HDD <br>
    - Use multiple CPU threads to prefetch data <br>

## Deep Learning Frameworks

### ThsorFlow

1. Rough Process: <br>
    - First define computational graph; <br>
    - Then run the graph many times <br>
2. Detail Process: <br>
    - Placeholders: `x = tf.placeholder(tf.float32, shape=(rx,lx)); y, w1, w2` similar <br>
    - Def forward pass: compute prediction for y and loss (No computation happens, just building model) `h, y_pred, loss` <br>
    - Tell TF to compute loss of gradient (No computation happens, just building model) `grad_w1, grad_w2` <br>
    - Enter a session so we can actually run the graph `with tf.Session() as sess:` <br>
    - Create numpy arrays that will fill in the placeholders above `Value = {x:..., w1:..., ...}` <br>
    - Run the graph: feed in the numpy arrays; get the output we want <br>
    `out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)` <br>
    `loss_val, grad_w1_val, grad_w2_val = out` <br>
    - Use to update and repeat the step 'R' <br>
3. Problems: Copying weights between CPU/GPU each step, Expensive! Solution:
    - Instead of placeholder, use Variable for `w1 w2` at step 'P' and tell TF how we want them to be initialized (Still, dont actually initialize now) `w1 = tf.Variable(tf.random_normal((rw, lw)))` <br>
    - Before the step 'E' and after the step 'T', add assign operations to update `w1, w2` as part of the computational graph `new_w1 = w1.assign(w1 - learning_rate * grad_w1)` <br>
    - After the step 'E' <br>
        ```py 
            sess.run(tf.global_variables_initializer())
            values = {x: np.random.randn(rx, lx), y: np.random.randn(ry, ly)}
            for t in range(TotStep):
                loss_val, = sess.run([loss], feed_dict=values)
        ```
4. But this will not update, because we dont tell the TF to update, we just tell it to calc loss. Solution: <br>
    - A funny way of doing so: <br>
        - We add a dummy node which will return 'None' at the step 'B': `updates = tf.group(new_w1, new_w2)` <br>
        - Last line change to `loss_val, _ = sess.run([loss, updates], feed_dict=values)` <br>
    - A way that TF provides: <br>
        - Instead step 'T' and 'B', we can just use an optimizer to cumpute gradients and update weight <br>
        `optimizer = tf.train.GradientDescentOptimizer(1e-5)` (1e-5 is the training_rate) <br>
        `updates = optimizer.minimize(loss)` <br>
        - Last line change to `loss_val, _ = sess.run([loss, updates], feed_dict=values)` <br>
5. TF also packing layer for us: After setting `x, y` , we can just call the following code to set layers without announcing `w`'s and `b`'s outslves: <br>
    ```py
    init = ty.contrib.layers.xavier_initializer()
    h = tf.layers.dense(inputs=x, units=H, activtion=tf.nn.relu, kernel_initializer=init)
    y_pred = tf.layers.dese(inputs=h, units=lx, kernel_initializer=init)
    ```
<br>
6. Keras: High-Level Wrapper <br>
7. Other High-Level Wrappers: TFLearn, TensorLayer, Sonnet, etc. <br>
8. Pretrained Models, Tensorboard, Distributed Version <br>
<br>
9. Like Theano <br>

### PyTorch

1. Three Levels of Abstraction <br>
    - Tensor: Imperative ndarray, but runs on GPU (like the Numpy array for TF) <br>
    - Variable: Node in a computational graph; stores data and gradient (like Tensor, Variable, Placeholder for TF) <br>
    - Module: A neural network layer; may store state or learnable weight (like tf.layers, or TFSlim, or TFLearn, or Sonnet, or... for TF) <br>
2. To let Tensor run on GPU, we need to use special datatype: `dtype = torch.cuda.FloatTensor` <br>
3. `x = torch.randc(rx, lx).type(dtype)` for use Tensor <br>
`x = torch.autograd.Variable(torch.randn(rx, lx), requires_grad=False)` for use Variable, and then it contain `x.data` , and a bool `x.grad` <br>
But their API is all the same <br>
But as using Variable, we can simply call `loss.backward()` to calc the gradients (dont forget to initialize 0 first **???**) <br>
4. nn package, optim package and DataLoaders function <br>
5. We can define our own new autograd functions and new modules in nn <br>
6. Pretrained Models, Visdom<br>
<br>
7. Update from Torch <br>
<br>
8. **Static vs Dynamic Graphs** <br>
    - Def: <br>
        - Static: TF, we first build then run <br>
        - Dynamic: PT, we just define Tensor or Variable first, and the build is done with in the loop and is running itself <br>
    - Optimization: Static ones can allow the computer to optimize the graph for you before it runs <br>
    - Serialization: Static ones once built, can serialize it and run it without the code that built the graph (which allow us to change coding environment) <br>
    - Conditional Operation: Dynamic ones will end up more clean. For Static you need special TF control flow to do so (like `tf.cond`) <br>
    - Loops: Like `y_t = (y_(t-1) + x_t) * w` , maybe even have a different size. Again, Dynamic ones are cleaner, and For Static you need special TF control flow to do so (in thsi case `tf.foldl` ) <br>
9. TensorFlow Fold: Dynamic graphs in TF <br>

### Caffe & Caffe2

<br>
<br>

# CNN Architectures

1. Review: LeNet 5 <br>
<br>
2. AlexNet: <br>
    - Architectures: <br>
    CONV1 (96 x 11 x 11 (x 3); stride 4 pad 0 => 35K parameters); <br>
    MAX POOL1 (3 x 3 (x 1); stride 2 => No parameter); <br>
    NORM1; <br>
    CONV2 (256 x 5 x 5 (x 48); stride 1 pad 2); <br>
    MAX POOL2 (3 x 3 (x 1); stride 2 => No parameter); <br>
    NORM2; <br>
    CONV3 (384 x 3 x 3 (x 128); stride 1 pad 1); <br>
    CONV4 (384 x 3 x 3 (x 192); stride 1 pad 1); <br>
    CONV5 (256 x 3 x 3 (x 192); stride 1 pad 1); <br>
    MAX POOL3 (3 x 3 (x 1); stride 2 => No parameter); <br>
    FC6 (4096 neurons); <br>
    FC7 (4096 neurons); <br>
    FC8 (1000 neurons); <br>
    Softmax <br>
    - First use of ReLU; <br>
    Heavy data augmentation; <br>
    Dropout; Momentum; Batch; Decay (Weight, Learning rate); Ensembles; etc. <br>
3. ZFNet: Change of hyperparameters <br>
<br>
4. VGGNet: Small filters, Deeper networks (VGG16, VGG19) <br>
    - only 3 x 3 CONV stride 1 pad 1 & 2 x 2 MAXPOOL stride 2 <br>
    That will lead to deeper, more non-linearities and less parameters <br>
    - Dont use NORM layers <br>
    - Most memory is in early CONV <br>
    Most parameters are in late FC (but not the last one) <br>
<br>
5. GoogLeNet: Deeper networkd, with computational efficiency <br>
    - No FC layers => Only 5M parameters <br>
    - Efficient "Inception" module: <br>
    Apply parallel filter operations (different size of CONV and Pooling) on the same input from previous layer; Concatenate all filter outputs together depth-wise <br>
    Usually CONV 1x1, 3x3, 5x5 and POOL 3x3 <br>
    - Problem: expensive for compute & always deeper because pooling layer, Solution: <br>
    use 1x1 conv (Bottleneck layers) to prjects depth to lower dimention (Before 3x3, 5x5 CONV and after 1x1 POOL) <br>
    <br>
    - Architecture: <br>
        - Stem Network (Conv-Pool-2x Conv-Pool) + Stacked Inception module + Classifier output (Without the expensive FC, which means there's only one FC to generate the score of each class) <br>
        - Auxiliary classification putputs to inject additional gradient at lower layers (AvgPool-1x1Conv-Fc-FC-Softmax) <br>
<br>
6. ResNet: Very deep networks using residual connections <br>
    - The deeper network sometimes perform worse than shallow ones in both training set and test set (so it's not caused by overfitting) <br>
    Because it is much more difficult to train and optimize <br>
    <br>
    - But deeper ones should at least perform as well as shallow ones by copying and add identity maping --- The basic idea of ResNet <br>
    - Solution: Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping <br>
    `X (input) -> CONV -> ReLU -> CONV -> F(X)` , and instead of using F(X) as input of next layer, we use F(X) + X. By this mean, F(X) is the residual of delta from X <br>
    <br>
    - Architecture: <br>
        - Stack residual blocks <br>
        - Periodically, downsampling by using stride 2 <br>
        - Addtional CONV layer at the beginning and only one FC layer at the end <br>
        - Also use bottleneck layer to improve efficiency <br>
    - Others: Batch Norm, Xavier/2, Momentum, Decayings (Weight, Learning rate), No dropout <br>
<br>
7. Other Architecture: <br>
    - Network in Network (NiN) <br>
    - Identity Mappings in Deep Residual Networks <br>
    - Wide Residual Networks <br>
    - Aggregated Residual Transformations for Deep Neural Networks (ResNeXt) <br>
    - Deep Networks with Stochastic Depth <br>
    - FractalNet: Ultra-Deep Neural Networks without Residuals <br>
    - Densely Connected Convolutional Networks <br>
    - SqueezeNet: AlexNet-level Accuracy With 50x Fewer Parameters and <0.5Mb Model Size <br>

<br>
<br>

# Recurrent Neural Networks

1. "Vanilla" Neural Networks: one to one <br>

2. Recurrent Neural Networks: <br>
    ONe to one; One to many; Many to one; Many to many <br>
    <br>

3. Recurrence formula: 
<br> $$
  h_t = f_W(h_{t-1}, x_t)
$$
   <br>
  `x -> RNN -> y` <br>
  `.....| ^.....` <br>
  `.....|_|.....` <br>
  Note W is the same for all the h_t <br>

4. Backpropagation through time <br>
    - Problem: if input is very large, it'll get super slow <br>
    - Improvement: Truncated Backpropagation through Time: <br>
    Run forward and backward through chunks of the sequence instead of whole sequence; <br>
    Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps <br>
    <br>

5. Example 1: (Vanilla) Recurrent Neural Network <br>
    - <br> $$
        h_t = \tanh(W_{hh} \times h_{t-1} + W_{xh} \times x_t)\\
        y_t = W_{hy} \times h_t
        $$

        <br>

    - The spread form of conputational graph <br>
      `x_i's, y_i's, h_i's, W, L_i's, L` <br>

    - The case of o-m, m-o, m-m; <br>
      Also m-m can be seen as m-o + o-m (kind of like coding and decoding) <br>

    - Example: Character-level Language Model <br>
        - Train by a sequence of characters you want <br>
        - Sample a character, get an output, generate the next character by argmax of prob, and feed in the model <br>

6. Example 2: Image Captioning <br>
    - Convolutional NN + Recurrent NN <br>
    - For CNN, we use the 4096 output before the final FC layers as the image input `v` to the RNN <br>
    - For RNN: <br>
        - Special \<START\> token and \<END\> token <br>

        - 
          <br> $$
          h = \tanh(W_{xh} \times x + W_{hh} \times h + W_{ih} \times v)
          $$
           <br>

        - Word-level <br>
    - To train it, we can train both CNN and RNN togather by pass down the gradient to CNN <br>
    - Improvement: With Attention (Soft & Hard) <br>
    RNN focuses its attention at a different spatial location when generating each word (and the input from t-1 and ConvNet to the RNN will be different) <br>

7. Example 3: Visual Question Answering <br>
    <br>

8. Multilayer RNN: <br>
<br> $$
  H^l_t = (h^{l-1}_t, h^l_{t-1})^T\\
  h^l_t = \tanh(W_l \times H^l_t)
$$
  <br>

9. LSTM <br>
    - Exploding & Vanishing Gradients Problem: Because when we do backprop, we'll multiply some `W` over and over again <br>
        - For EGP, nasty solution is to set a threshold, scale gradient if its norm is greater than it <br>
        - For VGP, Change RNN architecture ==> LSTM (which can solve both the problem) <br>

    - Formula: <br>
      <br> $$
      (i, f, o, g)^T = (\sigma, \sigma, \sigma, \tanh)^T \times W \times (h_{t-1}, x_t)
      $$
       (where sigma is sigmoid functon, and `W` is `4het * 2het` if the heights of `h, x` are both `het`) <br>
      <br> $$
      c_t = f .* c_{t-1} + i .* g
      $$
       <br>
      <br> $$
      h_t = o .* \tanh(c_t)
      $$
       <br>
      where `.*` is element-wise multiply <br>

    - Input gate (i): whether to write to cell <br>
      Forget gate (f): whether to erase cell <br>
      Gate gate (g): how much to write to cell <br>
      Output gate (o): how much te reveal cell <br>
      Where cell --- c <br>

    - Backprop: <br>
    Consider the path through cells (c's), then it only get element-wise muitiplied by different forget gates (f), and that's a lot nicer <br>
    **???** <br>
    And f usually initialize to near 1 to avoid vanishing at the beginning of the train <br>

10. Highway Networks <br>
    <br>

11. Other RNN Variants <br>

     - GRU <br>

<br>
<br>

# Detection, Segmentation and Locolization

## Semantic Segmentation
Want to produce a category label for each pixel of the input image <br>

1. Only pixels dont have the view of objects, so dont distinguish two cows for example, just give the label of cow <br>
<br>
2. Idea 1: Sliding Window <br>
Seperated to crops, and run a CNN on each crops to dicide the central pixel's label <br>
Very inefficient! <br>
3. Idea 2: Fully Convolutional <br>
    - Without FC layers, the output of the final Conv layer get a depth of class number and just view it as the scores of each class, and run the training <br>
    Super computational expensive <br>
    - Imporvement: <br>
    Design network as a bunch of convolutional layers, with downsampling and upsampling inside the network <br>
<br>
4. In-Network upsampling: <br>
    - Unpooling <br>
        - Nearest Neighbor <br>
        For 2x2 stride 2, what we do is to copy a element and fill the 2x2 with its number <br>
        `11 <- 1` <br>
        `11` <br>
        - Nails upsampling <br>
        For 2x2 stride 2, what we do is to fill the 2x2 with 0, except for one element <br>
        `10 <- 1` <br>
        `00` <br>
        - Max Unpooling <br>
        Just do the Nails upsampling and do it kind of symmetric with its corresponding downsampling <br>
        `12 -> 5 -> ... -> 1 -> 00` <br>
        `35.....................01` <br>
    - Transpose Convolution <br>
    We take one pixel in the input, multipliy it with the filter and add it to the output matrix <br>
    So the stride is the step length of the filter on the output matrix, and the pad is also using at the output matrix <br>

## Classification + Localization

5. Treat the localization as a regression prblem, using the same second to last FC layer as classification. <br>
While it go through another FC to the generate the class scores, let it fo through another FC layer to generate Box Coordinate `(x, y, w, h)` <br>
Also we'll have two loss functions: Softmax loss (or Cross-entropy loss) and Regression loss (L2 or L1 or others). So we take the weighted sum (hyperparameter) of them and use it to do the SGD <br>
Fancier: generate boxes for every class and only use the one of the true class to calc the second loss function <br>
<br>
6. Aside: using the idea of generate fix location <br>
    - Human pose Estimation <br>

## Object Detection

7. Might be varying number of output in every image <br>
<br>
8. Idea 1: as classification: Sliding Window <br>
Apply a CNN to many different crops of the image, CNN classifies each crop aas objects or background <br>
Problem: Need to apply CNN to huge number of locations and scales, Very computational expensive <br>
9. Idea 2: Region Proposals <br>
Give regions where the objects are likely to be found <br>
It is not deep learning, but a troditional machine learning <br>
<br>
10. R-CNN: combine the two ideas <br>
    - Steps:
        - Use Region Proposals to generate around 2K Regions of Interest (RoI) <br>
        - Warped them into a fix size the CNN wants <br>
        - Run each of them to the same CNN <br>
        - Do the Classification and Localization using the CNN <br>
        Note that we could train the CNN to Localize the object a little of the RoI, when the RoI miss a head of a person for example <br>
    - Problems: <br>
        - Super slow still, for both training and testing <br>
11. Fast R-CNN <br>
    - Step: <br>
        - Forward whole image through some conv. layers to get a high resolution convolutional feature map <br>
        - Use Region Proposals on the convolutional feature map to generate RoIs <br>
        - RoI Pooling layer to warped them into a fix size the CNN wants <br>
        - Directly run them to some FC Layers to do the Classification and Localization <br>
    - Super fast that Region Proposal becomes the bottleneck of speed <br>
12. Faster R-CNN <br>
    - Idea: Make CNN do proposals. Insert Region Proposal Network (RPN) in the CNN to predict proposals from features <br>
    - The RPN also do things like classification (over two class: objects or not) and Bounding-box regression, so it has two loss function, too. <br>
    Therefore, in total, we have 4 loss function that we need to use at the same time (weighted sum) to train the network <br>
<br>
13. Detection eithour Proposals: YOLO/SSD (You Only Look Once & Single Shot Detection) <br>
    - Idea: <br>
        - Divided into grid cell (R * L) <br>
        - For each grid cell, draw several (denote as `B` ) boxes you set centered it <br>
        - Within each grid cell <br>
            - Regress from each of the B base boxes to a final box with 5 numbers: `(dx, dy, dh, dw, confidence)` <br>
            - Predict scores for each of C classes (including background as a class) <br>
        - Output: Tensor R * L * (5 * B + C)
<br>
14. Aside: Object Detection + Captioning = Dense Captioning <br>

## Instance Segmentation
Kind of like Semantic Segmentation + Object Detection <br>

15. Mask R-CNN: <br>
    - Image -> CNN -> RoI Align -> Two branches: One Conv of Mini Semantic Segmentation; One do the Classification and Localization (and joint locolization for next no.) <br>
16. Also can do pose estimate at the same time <br>

<br>
<br>

# Visualizing and Understanding

1. First Layer (Generally Conv. layer) : Visualize Filters: Oriented edges and Opposing colors (Scale to 0~255) <br>
2. For intermediate layers: Directly using gray scale to interprete the weight matrix can not give much of understanding, we need more fancier method <br>
3. For last hidden layer: <br>
    - Nearest Neighbors <br>
    - Dimensionality Reduction: Use to high-D vectors to 2-D coordinates <br>
        - PCA <br>
        - t-SNE (t-distributed stochastic neighbor embeddings) <br>
4. For intermediate layers: While visualizing weight matrix is less useful, visualizing the activation maps of those intermediate layers is kind of interpretable <br>
5. Maximally Activating Patches: Visualizing what types of patches from input images cause maximal activation in different neurons <br>
6. Occlusion Experiment: Mask part of the image before feeding to CNN, draw heatmap of probability at each mask location <br>
7. Saliency Maps ===> Segmentation without supervision <br>
8. (Guided) Backpropagation <br>
9. Gradient Ascent <br>
    - Using gradient to change pixels to maximize the output score of some neuron <br>
    - Regularization: <br>
        - L2 norm <br>
        - Gaussian blur image (Periodical) <br>
        - Clip pixels with small values to 0 (Periodical) <br>
        - Clip pixels with small gradients to 0 (Periodical) <br>
    - Adding "multi-faceted" visualization (with more careful regularization, center-bias) <br>
<br>
10. Fooling Images / Adversarial Examples (**A whole lecture later**) <br>
    - Start from an arbitrary image <br>
    - Pick an arbitrary class <br>
    - Modify the image to maximize the class <br>
    - Repeat until network is fooled <br>
<br>
11. DeepDream: Amplify existing features <br>
    - Forward: compute activations at chosen layer <br>
    - Set gradient of chosen layer equal to its activation <br>
    - Backward: compute gradient on image <br>
    - Update image <br>
12. Feature Inversion: Given a CNN featrue vector for an image, find a new image that <br>
    - Matches the given feature vector <br>
    - (Start with noise) "looks natural" (by using regularization) <br>
<br>
13. **Total Variation Regularizer** <br>
<br>
14. Texture Synthesis <br>
    - Nearest Neighbor (without neural network) <br>
    - Neural Texture Synthesis: Gram Matrix <br>
15. Neural Artwork Synthesis <br>
16. Neural Style Transfer: Feature Inversion + Gram Matrix <br>
    - Content Image + Style Image <br>
    - Play with the hyperparameters <br>
    - Super slow <br>
    - Solution: Train another neural network to perform style transfer for us <br>
<br>
17. The apply of multi-scale processing on some of the above <br>

<br>
<br>

# Generative Models

1. Unsupervised learning <br>
Learn some underlying hidden structure of the data (Without label) <br>
Example: Clustering, Dimensionality reduction, Frature learning, Density
2. Generative Models: Given training data, generate new samples from the same distribution <br>
    - Two main classes: Explicit density; Implicit density <br>

## PixelRNN and PixelCNN

3. Explicit density model <br>
    Using chain rule to decompose likelihood of an image x into product of 1-d distributions: <br>
<br> $$
  p(x) = p^i_i(p(x_i | x_1,..., x_{i-1}))
$$
   <br>
  Then maximize likelihood of training data <br>

4. Complex distribution over pixel values on the right of the equation ==> Express using a neural network <br>
    We'll need to define ordering of "previous pixels" <br>
    <br>

5. PixelRNN: <br>
    - Generate image pixels startion from corner, and moving to right and down each step <br>
    - Dependency on previous pixels modeled using an RNN (LSTM) <br>
    - Drawback: Sequential, generation is slow <br>

6. PixelCNN: **???** <br>
    - Still generate image pixels startion from corner, and moving to right and down each step <br>
    - Dependency on previous pixels now modeled using a CNN over context region <br>
    - Training: maximize the likelihood of training images <br>
    - Training is faster. But generation must still proceed sequentially, so still slow <br>

## Variational Autoencoders (VAE) **??? from github**

7. Define intractable density function with latent z: <br>
<br> $$
  p_\theta(x) = \int(p_\theta(z) \times p_\theta(x|z))dz
$$
   <br>
  Cannot optimize directly, derive and optimize lower bound on likelihood instead <br>

8. Some background: Autoencoders <br>
    - Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data (x -> z, with dimensionality reduction) <br>
    - Encoder and Decoder: Linear + nonlinearity, Deep, Fully-connected, ReLU CNN <br>
    - Train such that features can be used to reconstruct original data <br>
    "Autoencoding" - encoding itself <br>
    Loss function: e.g. L2 <br>
    - After training, we can throw away the decoder layers, and use z to train a supervised model (Classifier etc.) <br>
    <br>

9. Variational Autoencoders <br>
    - Assume training data is generated from underlying unobserved (latent) representation `z` <br>
    - Sample from true prior `p_\theta^*(z)` , and then sample from true conditional `p_\theta^*(x|z_i)` <br>
      We want to estimate the true parameters `theta^*` of this generative model <br>
    - How should we represent this model <br>
      Choose prior `p(z)` to be simple, e.g. Gaussian <br>
      Conditional `p(a|z)` is complex => represent with neural network (Called Decoder network)<br>
    - How to train the madel <br>
        - Learn model parameters to maximize likelihood ==> `\int` is intractable <br>

        - Solution: In addition to decoder network modeling `p_theta(x|z)`, define additional encoder network `q_phi(z|x)` that approximates `p_theta(z|x)` <br>
          `p => miu_x|z, Sigma_x|z => x|z ~ hN(miu_x|z, Sigma_x|z)` <br>
          `q => miu_z|x, Sigma_z|x => z|x ~ hN(miu_z|x, Sigma_z|x)` <br>

        - Then for likelihood: 

        - <br> $$
            \log(p_\theta(x_i)) = E_{z~q_phi}(z|x_i)[\log(p_\theta(x_i))]\\
            = E_z[\log(p_\theta(x_i|z) \times p_\theta(z) / p_\theta(z|x_i))]\\
            = E_z[\log(p_\theta(x_i|z))] - E_Z[\log(q_\varphi(z|x_i) / p_\theta(z))] + E_z[\log(q_\varphi(z|x_i) / p_\theta(z|x_i))]\\
            = E_z[\log(p_\theta(x_i|z))] - D_{KL}(q_\varphi(z|x_i) || p_\theta(z)) + D_{KL}(q_\varphi(z|x_i) || p_\theta(z|x_i))\\
            \geq E_z[\log(p_\theta(x_i|z))] - D_{KL}(q_\varphi(z|x_i) || p_\theta(z))
            $$

             <br>

        - Then we get a loweer bound of `log(p_theta(xi))` which is `hL(xi, theta, phi) = E_z[log(p_theta(xi|z))] - D_KL(q_phi(z|xi) || p_theta(z))` <br>
          And when training, we maximize the lower bound: `theta^*, phi^* = argmax(sum_i(hL))` <br>

        - For a forward pass: <br>
            - input minibatch of x <br>
            - `q_phi(z|x)` Encoder network <br>
            - Sample `z` from `z|x ~ hN(miu_z|x, Sigma_z|x)` <br>
            - `p_theta(x|z)` Decoder network <br>
            - Sample `x|z` from `x|z ~ hN(miu_x|z, Sigma_x|z)` <br>
            - Calc `hL` and maximize it (backprop) <br>
    - For generating data: <br>
        - Use decoder network <br>
        - Now sample `z` from prior ( `z ~ hN(0, I)` ) <br>
    - Cons: <br>
        - Maximize the lower bound, okay but not as good <br>
        - Samples blurrier and lower quality picture <br>

## GANs (Generative Adversarial Networks)

10. Dont work with explicit distribution <br>
    Want to sample from complex, high-d training distribution <br>
    There's no direct way to do this ==> Want to have a transformation which tansform them into a simple distribution ==> Neural network <br>
    What we do is to sent a random noise (which is a simple distribution's sample) to a neural network, and let it to output a sample from training distribution <br>
    <br>

11. Two-player game: <br>
    - Generator network: try to fool the discriminator by generating real-looking images (from random noise) <br>
    - Discriminator network: try to distinguish between real and fake images <br>

12. Train jointly in minimax game: <br>
    <br> $$
    \min_{\theta_g}(\max_{\theta_d}(E_{x~p_{data}}[\log(D_{\theta_d}(x))] + E_{z~p(z)}[\log(1 - D_{\theta_d}(G_{\theta_g}(z)))] ))
    $$
    <br>
    where `D_{\theta_d}` is discriminator output, and `G_{\theta_g}` is generated fake data <br>
    <br>

13. Training GANs: Alternate between <br>
    - Gradient ascent on discriminator <br>
      `
      <br> $$
      \max_{\theta_d}(E_{x~p_{data}}[\log(D_[\theta_d](x))] + E_{z~p(z)}[\log(1 - D_{\theta_d}(G_{\theta_g}(z)))])
      $$
      `

    - Gradient descent on generator <br>
      <br> $$
      \min_{\theta_g}(E_{z~p(z)}[1 - D_{\theta_d}(G_{\theta_g}(z))])
      $$
       <br>
      But this function's slope is bad, cause we'll train slow when it's bad and fast when it's good <br>
      Using Gradient ascent on generator, instead <br>
      <br> $$
      \max_{\theta_g}(E_{z~p(z)}[D_{\theta_d}(G_{\theta_g}(z))])
      $$
       <br>

14. Jointly training two networks is challenging and can be unstable. Active research area <br>
    <br>

15. Improving: Convolutional Architectures <br>

16. Interpolating and Interpretablility <br>

17. Cons: 
    - Trickier / more unstabel to train <br>
    - Can't solve inference queries such as `p(x)` , `p(z|x)` <br>

<br>
<br>

# Deep Reinforcement Learning

1. Problems involving an agent interaction with an environment which provides numeric reward signals. Goal is to learn how to take actions in order to maximize reward <br>
Environment gives a State `s_t` to Agent, Agent generates back an Action `a_t` to Environment, then Environment gives a Reward `r_t` and the next State `s_(t+1)`. Repeat until the Environment gives the terminal State <br>
<br>
2. Examples: <br>
    - Cart-Pole Problem <br>
    - Robot Locomotion <br>
    - Atari Games <br>
    - Go (围棋) <br>

## Markov Decision Process  (MDP)

3. It is mathematical formulation of the PL problem <br>
    <br>

4. Markov Property: Current state completely characterises the state of the world <br>
    <br>

5. An MDP is defined by: `(hS, hA, hR, P, gamma)` <br>
    - `hS` : set of possible states <br>
    - `hA` : set of possible actions <br>
    - `hR` : distribution of reward given (state, action) pair <br>
    - `P` : transition probability i.e. distribution over next state given (state, action) pair <br>
    - `gamma` : discount factor (how much we value rewards coming up soon versus later on) <br>

6. The MDP: <br>
    - At time step `t = 0`, environment samples initial state `s_0 ~ p(s_0)` <br>
    - Then, for `t = 0` until done: <br>
        - Agent selects action `a_t` <br>
        - Environment samples reward `r_t ~ hR(. | s_t, a_t)` <br>
        - Environment samples next state `s_(t+1) ~ P(. | s_t, a_t)` <br>
        - Agent receives reward `r_t` and next state `s_(t+1)` <br>
    <br>
    - A policy `pi` is a function from `hS` to `hA` that specifies what action to take in each state <br>
    - Objective: find policy `pi^*` that maximizes cumulative discounted reward: `sum_t(gamma^t * r_t)` <br>

7. Example: Grid World <br>

8. How do we handle the randomness (initial state, transition probability...) <br>
    Solution: Maximize the expected sum of rewards <br>
<br> $$
  p_i^* = \arg \max(E[\sum_t(\gamma^t \times r_t) | p_i])
$$
   with `s_0 ~ p(s_0), a_t ~ pi(. | s_t) ,s_(t+1) ~ p(. | s_t, a_t)` <br>

## Q-Learning

9. Def: Value function and Q-value funciton <br>
    - Value function at state `s` , is the expected cumulative reward from following the policy from state s: <br>
      <br> $$
      V_{p_i}(s) = E[\sum_t(\gamma^t \times r_t) | s_0 = s, p_i]
      $$
       <br>

    - Q-value function at state `s` and action `a` , is the expected cumulative reward from taking action `a` in state `s` and then following the policy: <br>
      <br> $$
      Q_{p_i}(s, a) = E[\sum_t(\gamma^t \times r_t) | s_0 = s, a_0 = a, p_i]
      $$
       <br>

    - `
      <br> $$
      Q^*(s, a) = \max_{p_i}(E[\sum_t(\gamma^t \times r_t) | s_0 = s, a_0 = a, p_i])
      $$
       <br>

10. Bellman equation: <br>
    <br> $$
    Q^*(s, a) = E_{s'~\varepsilon}[t + \gamma \times \max_{a'}(Q^*(s', a') | s, a)]
    $$
     <br>
    <br>

11. Solving for the optimal policy: Value iteration algorithm (use Bellman equation as an iterative update) <br>
    <br> $$
     Q_{i+1}(s, a) = E[r + \gamma \times \max_{a'}(Q_i(s', a')) | s, a]
    $$
      <br>
     `Q_i` will converge to `Q^*` as `i -> infinity` <br>

12. Problem: Not scalable. Must compute `Q(s, a)` for every state-action pair. Computationally infeasible <br>
     Solution: Use a function approximator to estimate `Q(s, a)`. E.g. a neural network <br>
     <br>

13. Deep Q-Learning <br>
     - Q-Learning: Use a function approximator to extimate the axtion-value function `Q(s, a; theta) ~= Q^*(s, a)` <br>
       Deep Q-Learning: When the function approximator is a deep neural network <br>

     - Forward Pass <br>
       Loss function: 
       <br> $$
       L_i(\theta_i) = E_{s,a~\rho(.)}[(y_i - Q(s, a; \theta_i))^2]
       $$
        <br>
       Where 
       <br> $$
       y_i = E_{s'~\varepsilon}[r + \gamma \times \max_{a'}(Q(s', a'; \theta_{i-1})) | s, a]
       $$
        <br>

     - Backward Pass <br>
     Gradient update (with respect to Q-function parameters `theta`) <br>

14. Example: Playing Atari Games <br>

15. Experience Replay: <br>
    - Learning from batches of consecutive samples is problematic: <br>
        - Samples are correlated => inefficient learning <br>
        - Current Q-network parameters determines next training samples => can lead to bad feedback loops <br>
    - Address these problems using experience replay: <br>
        - Continually update a replay memory table of transitions `(s_t, a_t, r_t, s_(t+1))` as game (experience) episodes are played <br>
        - Train Q-network on random minibatches of transitions from the replay memory, instead of consecutive samples <br>
        - As each of the transitions can also contribute to multiple weight updates => greater data efficiency <br>

## Policy Gradients

16. If dimension goes to really huge, the Q-function can be very complicated <br>
<br>
17. Policy Gradients: <br>
    - Formally, let's def a class of  parametrized policies: 
      <br> $$
      \Pi = {\pi_\theta, \theta\in \mathbb{R}^d}
      $$
       <br>

    - For each policy, def its value: 
      <br> $$
      J(\theta) = E[\sum_t(\gamma^t \times r_t) | \pi_\theta]
      $$
       <br>

    - We want to find the optimal policy 
      <br> $$
      \theta^* = \arg\max(J(\theta))
      $$
       ==> Gradient Ascent on policy parameters <br>
18. Reinforce algorithm <br>
    - Mathematically <br>
      <br> $$
      J(\theta) = E_{\tau~p(\tau;\theta)}[r(\tau)] = \int_\tau(r(\tau) \times p(\tau; \theta))d\tau
      $$
       <br>
      Where `r(\tau)` is the reward of a trajectory `tau = (s_0, a_0, r_0, s_1,...)` <br>

    - Directly `Delta` is Intractable. However, we can use a nice trick <br>
      <br> $$
      \nabla_\theta p(\tau; \theta) = p(\tau; \theta) \frac {\nabla_\theta p(\tau; \theta)}{p(\tau; \theta)} = p(\tau; \theta) \times \nabla_\theta \log p(\tau; \theta)
      $$
      <br>
      Then 
      <br> $$
      \nabla_\theta J(\theta) = E_{\tau~p(\tau;\theta)}[r(\tau) \times \nabla_\theta \log p(\tau; \theta)]
      $$
       <br>

    - Can we compute those quantities without knowing the transition probabilities <br>
      We have: 
      <br> $$
      p(\tau; \theta) = \prod_{t\geq0}(p(s_{t+1} | s_t, a_t) \times \pi_\theta(a_t | s_t))
      $$
      <br>
      Thus:
      <br> $$
      \log p(\tau; \theta)) = \sum_{t\geq0}(\log p(s_{t+1} | s_t, a_t) + \log \pi_\theta(a_t | s_t))
      $$
       <br>
      And when differentiating: 
      <br> $$
      \nabla_\theta\log p(\tau; \theta) = \sum_{t\geq0}(\nabla_\theta\log\pi_\theta(a_t | s_t))
      $$
       <br>
19. However, this also suffers from high variance because credit assignment is really hard <br>
20. Variance reduction: <br>
    - First idea: push up probabilities of an action only by the cumulative future reward from that state <br>
    - Second idea: Use discount factor `gamma` to ignore delayed effects <br>
    - Baseline: Introduce a baseline function dependent on the state, and rescale the rewards <br>
    Choose the base line: <br>
        - constant moving average of rewards experienced so far from all trajectories <br>
        - if this action was better than the expected value of what we should get from that state <br>
          Q-function and value function! <br>
          Intuitively, we are happy with an action `a_t`  in a state `s_t` if `Qpi(s_t, a_t) - Vpi(s_t)` is large <br>
          Def Advatage function `A(s, a) = Qpi(s_t, a_t) - Vpi(s_t)`
21. ===> Combine Q-Learning and Policy Gradients by training both an actor(the policy) and a critic (the Q-function)
<br>
22. Examples: <br>
    - Recurrent Attention Model (RAM) <br>
    -  fine-grained image recognition <br>
    - image captioning <br>
    - visual question-answering <br>
    - AlphaGo <br>

<br>
<br>

# Efficient Methods and Hardware for Deep Learning

## Algorithms for Efficient Inference

1. Pruning <br>
2. Weight Sharing <br>
3. Quantization <br>
4. Low Rank Approximation <br>
5. Binary / Ternary Net <br>
6. Winograd Transformation <br>

## Hardware for Efficient Inference

1. TPU <br>
2. EIE <br>

## Algorithms for Efficient Training

1. Parallelization <br>
2. Mixed Precision with FP16 and FP32 <br>
3. Model Distillation <br>
4. DSD: Dense-Sparse-Dense Training <br>

## Hardware for Efficient Training

<br>
<br>

# Adversarial Examples and Adversarial Training

1. Adversarial Examples:  <br>
    - Fooling image as we mentioned before <br>
    - Not just the neural network <br>
2. Understand why it happens: <br>
    - First idea: Causing from overfitting --- Wrong <br>
    Because it's not random, it is systematic over pictures and different trainings <br>
    - Second idea: Causing from underfitting <br>
    Excessive Linearity, and it force the point far away the dicition boundary to be confident classified even if we actually dont have any input there <br>
<br>
3. The Fast Gradient Sign Method <br>
<br>
4. The Clever Hans Theory <br>
<br>
5. RBFs: resist the problem, but it is very shallow and extramly difficult to train if it get deeper in order to increase its accuracy <br>
<br>
6. Transferability Attack <br>
    - Enhancing With Ensembles <br>
<br>
7. The Adversarial Examples for Human (视觉错觉) and current neural networks is very different and have almost no 重叠 <br>
Which means there's huge different in the mechanism of the two <br>
<br>
8. Adversarial Training <br>
    - Can improve a bit, but still long way to go <br>
    - Semi-supervised learning <br>