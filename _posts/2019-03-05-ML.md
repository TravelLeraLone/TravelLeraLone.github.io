---
layout: post
title: CS229学习笔记
tags: Notes ML FL
categories: Study
excerpt: 转行起始阶段的学习资料，CS229的详细笔记，多是机器学习的最基本的知识，其中很多内容非常数学而又点到为止，当时学习的时候由于时间紧迫更多的是跟随课程一样点到为止，到后来的学习实践过程中才发现其中的很多内容都是很有必要深入探究的，尤其是其数学本质以及推导过程。虽然现在有说法是越需要人进行数学推导的方法其效果越差，但是在研究的过程中，数学的基础还是起到了相当大的作用的，尤其是在non-trivial的改进方法探究以及最新的研究方向的探索初期等阶段。
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# Intoduction

1. What is machine learning? <br>
Experience E, task T, performance measure P. If its performance on T, as measured by P, improves with experience E. <br>
2. Types: <br>
    - Supervised learning & Unsupervised learning <br>
    - Reinforcement learning & recommender sysytems <br>
3. How to apply learning algorithm <br>
4. Basic knowledge of linear algebra <br>
    - Matrix: rows x columns <br>
    - Vector: An n x 1 matrix; 1-indexed and 0-indexed(The first element of vector y is called y1 or y0) <br>
    - Scalar means real number <br>
    - PredictionVector = DataMatrix x ParametersVector <br>
    - PredictionMatrix = DataMatrix x Multi_Pairs_ParametersMatrix
    - Not Commutative, Associative, Inverse(singular or degenerate), Transpose

## Supervised learning

5. Def: We give the computer a dataset with the correct answer of each of the data <br>
6. Regression: Predict continuous value output <br>
7. Classification: Discrete valued output <br>
8. Deal with infinite features: we have algorithm to do that <br>

## Unsupervised learning

9. Def: We just give the computer a dataset, and let the computer to find the structure of the data <br>
    - Type 1: Split the data into different groups <br>
    - Type 2: Audio seperating <br>

<br>
<br>

# Linear Regression

1. Training set -> Learning algorithm -> h: h(x) = y (hypothesis) <br>
    - x input, y output, m num of training examples <br>
    - h_theta(x) = theta0 + theta1 * x (called univariate linear regression) <br>
    - `h_theta(xi) = (xi)^T * theta` <br>

## LMS(Least Mean Square) Algorithm

2. Def Loss Function --- how to choose thetas <br>
    - The idea: minimize the saperation of h(x)s from ys in the training sets <br>
    - In this example `minimize(sum((h(xi) - yi) ^ 2) / (2m))` (what we minimize is also called Square Error Cost Function) <br>
    - Def cost function (J) as the function we try to minimize <br>
3. Understanding of Hypothesis and Cost Function <br>
    - J is a function of parameters; For fixed parameters, h is a function of x <br>
    - For each value of parameters, J([parameters]) corresponds to a specific hypothesis <br>
    - The high-D understanding of h & J <br>
4. Gradient Descent --- How to automatically find the parameters that minimize J <br>
    - Ganerally we have a func J <br>
        - Start with some parameters <br>
        - keep changing them to reduce J, until hopfully we end up at a minimum <br>
        - Gradient: the fast changing direction of a func <br>
        - `thetaj := thetaj - alpha * pd(J) / pd(thetaj)` , in which alpha is called learning rate, too small --- slow; too large --- may overshoot the minimum, fail to converge, or even diverge <br>
        - BTW: To simultaneously update all parameters correctly, we need to save all the new value in a register, and update them togather after we have done all the calculation. Otherwise, the parameter calculate later will not get the right value <br>
    - Specified to cost func J <br>
        - That's why there are '2' in the hypothesis <br>
        - `theta_j := theta_j + alpha * (yi - h(xi)) * xi_j` <br>
        - Convex function --- Boat shape <br>
        - Batch Gradient Descent: Each step of gradient descent uses all the training examples(the sum coverage); And there's other ways we'll talked about later. <br>
        - Stpchastic(Incremental) Gradient Descent: Each step uses samples of examples

## The normal equations

5. Some more Matrix calc <br>
    - Derivative: Df(A)=[pd(f) / pd(Aij)] <br>
    - Trace: trA(commutative) <br>
    - Some equation: <br>
        - D(_A)trAB = B^T <br>
        - D(_A^T)f(A) = (D(_A)f(A))^T <br>
        - D(_A)trABA^TC = CAB + C^TAB^T <br>
        - D(_A)|A| = |A|(A^-1)^T <br>
6. The Normal Equation: In order to let DJ(theta) = 0<br>
`X^T * X * theta = X^T * y` <br> <br>
7. So by `theta = (X^T * X)^-1 * X^T * y`, we can minimize J without resorting to an iterative lgorithm <br>

## probabilistic interpretation
8.  The prof of LMS: Minimizing J = Maximizing likelihood or log likelihood<br>
    - `L(theta) = pi(p(yi | xi; theta))` <br>
    - `l(theta) = log(L(theta))` <br>
    - In the example above, `y|x; theta ~ N(miu, sigma^2)` (N as Gaussian distribution) <br>

## Locally weighted linear regression(LWR)

9. Underfitting & Overfitting <br>
<br>
10. Suppose we have enough data that the choice of features is less critical <br>
11. In LWR instead of minimize `sum(yi - theta^T * xi)^2` , we minimize `sum(wi * (yi - theta^i * xi)^2)` , where wi's are non-negative valued weights <br>
12. Fairly standard choice of w <br>
`wi = e^(- (xi - x)^2 / (2 * tao^2))` <br>
where x is the query point, tao is called the bandwidth parameter <br><br>
13. Non-parametric

<br>
<br>

# Classification and logistic regression

For now, we'll focus on the binary classification problem, 0 --- negative class('-'); 1 --- positive class('+'). Also given xi calc yi is also called the label for the reaining example <br>

## Logistic regression

1. Treated as continuous values prediction <br>
2. Logistic function(also called Sigmoid function) <br>
`g(z) = 1 / (1 + e^-z)` <br>
Also `g'(z) = g(z) * (1 - g(z))`
3. Hypothesis: `h_theta(x) = g(theta^T * x)` <br>
4. Probabilistic assumption: <br>
    - P(y = 1 | x; theta) = h_theta(x) <br>
    - P(y = 0 | x; theta) = 1 - h_theta(x) <br>
    - Genarally: `p(y | x; theta) = (h_theta(x))^y * (1 - h_theta(x))^(1 - y)` (called Bernoulli distribution `Bernoulli(phi = h_theta(x))`) <br>
<br>
5. To maximize log likelihood `l(theta)` , we dirived the same fomula: <br>
`theta_j := theta_j + alpha * (yi - h_theta(xi)) * xi_j`

## Digression: The perceptron learning algorithm
- All the same as above but `g(z) = z<0?0:1` <br>

## Another algorithm for maximizing `l(theta)`

6. Newton's method to find a zero for a func: <br>
`theta := theta - f(theta) / f'(theta)` (linear func understanding) <br>
7. Apply to l(theta): `theta := theta - l'(theta) / l''(theta)` <br>
8. Considering high-D: (Newton-Raphson method) <br>
`theta := theta - H^-1 * Delta_theta(l(theta))` <br>
where `H = dp^2(l(theta)) / (dp(theta_i) * dp(theta_j))` <br>
<br>
9. Can understand as Gradient Descenting with a changing learning rate `alpha` <br>

<br>
<br>

# Generalized Linear Models(GLMs)

## The exponential family

1. The exponential family distributions: <br>
`p(y; yita) = b(y) * exp(yita^T * T(y) - a(yita))` <br>
where: <br>
    - yita --- natural parameter(also called canonical parameter); <br>
    - T(y) --- sufficient satistic; <br>
    - a(yita) --- log partition function( `e^(-a(yita))` essentially plays the role of a normalization constant) <br>
<br>
2. Example 1: For Bernoulli(phi): <br>
    - T(y) = y; <br>
    - a(yita) = - log(1 - phi) = log(1 + e^yita); <br>
    - b(y) = 1
3. Example 2: For Gaussian N(miu, sigma^2 = 1): *(Because sigma has no effect on the final choice of theta and h)* <br>
    - T(y) = y; <br>
    - a(yita) = miu^2 / 2 = yita^2 / 2; <br>
    - b(y) = exp(- y^2 / 2) / sqr_root(2 * pi) <br>
4. Other Examples: multinomial, Poisson, gamma, exponential, bata, Dirichlet, etc. <br>

## Constructing GLMs

5. Assumption: <br>
    - y | x; theta ~ ExponentialFamily(yita) --- the distribution of y given x and theta follow some exponential family distribution with parameter yita <br>
    - Given x, our goal is to predict the expected value of T(y) given x <br>
    In most of our examples, we will have T(y) = y, so this means we would like the prediction h(x) output by our learned hypothesis h to satisfy h(x) = E[y|x]（条件期望） <br>
    - The natural parameter yita and the inputs x are related linearly: <br>
    `yita_i = theta_i^T * x` <br>
6. Some special name: <br>
    - y --- Response variable <br>
    - Canonical response function & Canonical link function: h(x)(or saying g(yita)) & knowing outputs calc yita <br>
<br>
7. Ordinary Least Squares <br>
    - We model the conditional distribution of y given x as a Gaussian function above <br>
    - So we have `h_theta(x) = E[y | x; theta] = miu = yita = theta^T * x` <br>
8. Logistic regression <br>
    - Bernoulli <br>
    - `h_theta(x) = E[y | x; theta] = phi = 1 / (1 + e^-yita) = 1 / (1 + e^-(theta^T * x))` <br>
9. Softmax regression (nultinomial distribution) <br>
    - For a multinomial over k possible outcomes, we'll use k - 1 independent parameters to parameterize it `phi_1, phi_2, ..., phi_(k - 1)` <br>
    - `phi_i = p(y = i; phi)` and `p(y = k; phi) = 1 - sum(phi_i)` (let it = phi_k, where phi_k is not a parameter but just a notation) <br>
    - **A notation to introduce:** 1{·} as `"The arguement is true" ? 1 : 0`
    - To express the nultinomial as an exponential family distribution: <br>
        - we define that `(T(y))_i = 1{y = i}` (for i = 1, 2, ..., k-1) <br>
        That is: `T(i) = [0, ..., 1(i_th), ..., 0((k-1)_th)]^T ` (for i = 1,2,...,k - 1); `T(k) = [0,...,0]^T` <br>
        Moreover, E[(T(y))_i] = P(y = i) = phi_i <br>
        - `yita = [log(phi_1 / phi_k), ..., log(phi_(k-1) / phi_k)]^T` <br>
        - `a(yita) = - log(phi_k)` <br>
        - `b(y) = 1` <br>
    - The softmax function:
        - The link function: `yita_i = log(phi_i / phi_k)`; we also define `yita_k = 0` for convenience <br>
        - The response function: `phi_i = e^yita_i / sum(e^yita_j)` <br>
    - So that `p(y = i | x; theta) = phi_i = e^(theta_i^T * x) / sum(e^(theta_j^T * x))` <br>

<br>
<br>

# Generative Learning Algorithms

1. 
    1. Discriminative learning algorithms: try to learn `p(y|x)` direcstly or mappings directly from the space of inputs to the labels; <br>
    2. Generative learning algorithm: instead, try to model `p(y)` (called class priors) and `p(x|y)`, then calc `p(y|x)` by Bayes rule: <br>
    `p(y|x) = p(x|y) * p(y) / p(x)` <br>
    where `p(x) = sum(p(x|y = i) * p(y = i))` <br>
    Moreover, `arg max_y(p(y|x)) = arg max_y(p(x|y) * p(y) / p(x)) = arg max_y(p(x|y) * p(y))` <br>
    So likelihood function would be derived from `p(x|y)` and `p(y)` <br>

## Gaussian Discriminant Analysis (GDA)

2. Introduce: Multinariate normal distribution <br>
    - Mean vector miu (n); Covariance matrix Sigma (n x n) (>= 0, symmetric and positive semi-definite) <br>
    - `p(x; miu, Sigma) = exp(-(x - miu)^T * Sigma^-1 * (x - miu) / 2) / ((2 * pi)^(n / 2) * |Sigma|^0.5)` <br>
    - `E[X] = \jf\_x(x * p(x; miu, Sigma))dx = miu` <br>
    `Cov(Z) = E[(Z - E[Z])(Z - E[Z])^T] = E[ZZ^T] - E[Z]E[Z]^T = Simga` <br>
    - Standard normal distribution <br>
    - In Sigma, the Aij represent the degree of compressness towards the direction `xi = xj` (As it goes bigger, it become more spread out in the direction `xi = xj`, and more compress in the direction `xi = -xj` (expect when `i = j` )) <br>
<br>
3. The Gaussian Discriminant Analysis model: Take a binary-classification problem which vector x are continuous, real-valued vectors as an example <br>
    - y ~ Bernoulli(phi) <br>
    x | y = 0 ~ N(miu0, Sigma) <br>
    x | y = 1 ~ N(miu1, Sigma) <br>
    - Parameters: phi, miu0, miu1, Sigma (Note that only one Sigma for two miu) <br>
    - By maximize `l(phi, miu0, miu1, Sigma)`, we get: <br>
    `phi = sum(1{yi = 1}) / m` <br>
    `miu0 = sum(1{yi = 0} * xi) / sum(1{yi = 0})` <br>
    `miu1 = sum(1{yi = 1} * xi) / sum(1{yi = 1})` <br>
    `Sigma = sum((xi - miu_yi)(xi - miu_yi)^T) / m` <br>
4. Discussion: GDA & logistic regression <br>
    - Let `theta = theta(phi, Sigma, miu0, miu1)` then GDA become logistic regression <br>
    - GDA make a stronger assumption that not only `p(y|x)` follows logistic function, but also `P(x|y)` follows multivariate gaussian function <br>
    - When assumption correct, GDA is better, even asymptotically efficient <br>
    - GDA is more data efficient (requires less training data to learn "well") <br>
    - Logistic regression more robust
5. Asymptotically efficient: Means that in the limit of very large training sets (large m), there is no algorithm that is strictly better than GDA <br>

## Naive Bayes

6. Input vector x are discrete-valued <br>
7. Naive Bayes (NB) assumption: x_i's are conditionally independent given y <br>
Then `p(x_1, ..., x_50000 | y) = p(x_1 | y)p(X_2 | y, x_1)···p(x_50000 | y, x_1, ..., x_49999) = pi(p(x_j | y))` <br>
8. Naive Bayes classifier: <br>
    - Parameterize and Maximize likelihood: <br>
    `phi_(j|y=1) = p(x_j=1|y=1) = sum(1{xi_j=1 ^ yi=1}) / sum(1{yi=1})` <br>
    `phi_(j|y=0) = p(x_j=1|y=0) = sum(1{xi_j=1 ^ yi=1}) / sum(1{yi=0})` <br>
    `phi_y = p(y = 1) = sum(1{yi=1}) / m` <br>
    - Then `p(y = 1 | x) = p(x | y = 1)p(y = 1) / p(x)` where `p(x) = p(x | y = 1) + p(x | y = 0)` <br>
    - x can be multinomial; <br>
    For continuous x, if GDA etc. fit not very good, we can also consider to discretize it and use Naive Bayes <br>
9. Laplace smoothing: <br>
    - Problem of previous version: For words never come out, both `phi` would be `0`, and the output will become `0/0` <br>
    - Soution: Laplace smoothing <br>
    Generally `phi_j = (sum(1{zi = j}) + 1) / (m + k)` where `k` is the number of possible j values to make sure the sum of all phi is `1` <br>
    Specifically, `phi_(j|y=1) = (sum(1{xi_j=1 ^ yi=1}) + 1) / (sum(1{yi=1}) + 2)`, similarly for `phi_(j|y=0)` <br>
<br>
10. Multinomial event model <br>
x_i represent what the i_th word of the passage is; <br>
We consider all xi_j's distrubution is the same for the same i <br>

<br>
<br>

# Support Vector Machines

## Margins

1. Intuition: Margins, Confidence of the prediction, Separating Hyperplane <br>
Important assumption: Data is linearly separable <br>
2. For the next discussing, we let the two value that y could have be -1 and 1, and donote `h_(w,b)(x) = g(w^T * x + b)`, where `g(z) = z<0 ? -1 : 1` <br>
<br>
3. Functional Margin: `gammai = yi * (w^T * xi + b)` and `gamma = min(gammai)`<br>
    - \> 0 --- correct; Large --- confident <br>
    - There is problem that singly multiply 2 to w and b would result in the functional margin become bigger without doing anything meaningful <br>
    So often we will impose some sort of normalization condition such as `||w||_2 = 1` <br>
4. Geometric Margin: `gammai = yi * ((w / ||w||)^T * xi + b / ||w||)` and `gamma = min(gammai)` <br>

## The Optimal Margin Classifier

5. The natural desideratum: Maximizes the (geometric) margin <br>
So the problem is: `max_(gamma,w,b)(gamma / ||w||) s.t. yi * (w^T * xi + b) >= gamma, i = 1, ..., m` <br>
Which is for calc easier: `min_(w,b)(0.5 * ||w||^2) s.t. yi * (w^T * xi + b) >= 1, i = 1, ..., m` <br>
Can be considered as the loss function that the model (Optimal Margin Classifier) use to do the optimization <br>
<br>
6. Digression: Lagrange Duality <br>
    - Consider the problem: `min_w(f(w)) s.t. h_i(w) = 0, i = 1, ..., l` <br>
    条件极值， 拉格朗日乘数法: <br>
        - Def Lagrangian: `hL(w, beta) = f(w) + sum(beta_i * h_i(w))` where `beta_i`s are called Lagrange multipliers <br>
        - Then solve `pdhL / pdw_i = 0; pdhL / pdbeta_i = 0` to get `w` and `beta` <br>
    - Consider the problem (primal optimization problem): `min_w(f(w)) s.t. g_i(w) <= 0, i = 1, ..., k; h_i(w) = 0, i = 1, ..., l` <br>
        - Def generalized Lagrangian: `hL(w, alpha, beta) = f(w) + sum(alpha_i * g_i(w)) + sum(beta_i * h_i(w))` <br>
        - Consider the quantity: `theta_P(w) = max_(alpha, beta; alpha_i>=0)(hL(w, alpha, beta))` <br>
        we can dirive that `theta_P(w) = w satisfies primal constraints ? f(w) : infinity` <br>
        Then we have `min(theta_P(w)) = min(max(hL(w, alpha, beta)))` <br>
        we call the solution `p^* = min(theta_P(w))` the value of the primal problem <br>
        - Consider another problem called the Dual optimization problem: <br>
        First define `theta_D(alpha, beta) = min_w(hL(w, alpha, beta))` <br>
        Then the problem is `max_(alpha_i>=0)(theta_D(alpha, beta)) = max_(alpha_i>=0)(min(hf(w, alpha, beta)))` <br>
        we call the solution `d^* = max_(alpha_i>=0)(theta_D(alpha, beta))` the value of the primal problem <br>
        - Because `maxmin <= minmax` we get `d^* <= p^*`
        - Suppose `f` and `g_i`s are convex, and the `h_i`s are affine. Suppose further taht the constraints `g_i` are (strictly) feasible <br>
        Then we have `p^* = d^* = hL(w^*, alphs^*, beta^*)` <br>
        Moreover, they satisfy the **Karush-Kuhn-Tucker (KKT) conditions**. And The `w^*, alphs^*, beta^*` that satisfy KKT conditions are also a solution <br>
            - `pdhL(w^*, alphs^*, beta^*) / pdw_i = 0, i=1, ..., n` <br>
            - `pdhL(w^*, alphs^*, beta^*) / pdbeta_i = 0, i=1, ..., l` <br>
            - `alpha_i^* * g_i(w^*) = 0, i = 1, ..., k` (called the KKT dual complementarity condition) <br>
            - `g_i(w^*) <= 0, i = 1, ..., k` <br>
            - `alpha_i^* >= 0, i = 1, ..., k` <br>
<br>
7. So to match the Lagrange Duality expression, we write the problem as: `min_(w,b)(0.5 * ||w||^T) s.t. g_i(w) = - yi * (w^T * xi + b) + 1 <= 0, i = 1, ..., m` <br>
8. From the KKT we konw that only when `g_i(w) = 0` can `alpha != 0` <br>
That is to say, only when `yi * (w^T * xi + b)` get its minimum (which is 1 because of the constrain) <br>
To understand from a geomatric view, for a certain line (w, b choosen), we can do translation towards both directions, until it touch the point, these point it touches have a non-zero `alpha` <br>
9. Specifically in Optimal Margin Classifier, the points with non-zero `alpha` are called Support Vector <br>
<br>
10. To use Largrange Duailty and KKT: <br>
    - First we write down `hL(w, b, alpha) = 0.5 * ||w||^2 - sum(alpha_i * (yi * (w^T * xi + b) - 1))` <br>
    - Then we let the partial derivatives of hL to w and b be `0` (KKT 1 and also minimizing `hL` over `w, b` to get `theta_D`), and we get: <br>
    `w = sum(alpha_i * yi * xi)` <br>
    `sum(alpha_i * yi) = 0` <br>
    - So bring back to the origin function, we get: `hL(w, b, alpha) = sum(alpha_i) - 0.5 * sum(yi * yj * alpha_i * alpha_j * xi^T * xj) = W(alpha)` <br>
    - Then we get the dual optimization problem: `max_alpha(W(alpha)) s.t. alpha_i >= 0, i = 1, ..., m; sum(alpha_i * yi) = 0` <br>
    - **why fit KKT 3 ???**
    - If we can solve it, we then can get `w^*` through `w = sum(alpha_i * yi * xi)` and `b^*` through `b^* = -0.5 * (max_(i:yi=-1)(w^(*T) * xi) + min_(i:yi=1)(w^(*T) * xi))`
<br>
11. When the test `x` input, we need to calc `w^T * x + b = sum(alpha_i * yi * xi)^T * x + b`. Consider what we talk about in No.8, what we really need to do is to calc the inner product between input `x` and these Support Vectors. --- Support Vector Machine (SVM) <br>

## Kernels

12. For inputs x (called input attributes below), we may consider to perform training using the feature calculated from x (called input features below). And we denote `phi` the feature mapping (function mapping attributes and corresponding features) <br>
13. Because in the dual optimization problem, all we need is inner product between the input attributes x, we may instead want to use it (that is SVMs) to learn using these features `phi(x)` <br>
14. Def Kernel: `K(x, z) = phi(x)^T * phi(z)` <br>
15. Sometime, calc `K(x, z)` directly cost much less time than calc `phi(x)` first, because `phi(x)` usually are high-D vectors <br>
Example: `K(x, z) = (x^T * z + c)^d` <br>
16. An understanding of Kernels: A measurement of how similar are `phi(x)` and `phi(z)`, or of how similar are x and z <br>
    - So we may directly write down the `K(x, z)` as what we think might be a reasonable measure of how similar x and z are <br>
    - Moreover, to test whether the function we write can be a Kernel function --- Theorem (Mercer) <br>
    K is a valid (Mercer) kernel if and only if its corresponding matrix K (Kij = K(xi, xj)) is symmetric positive semi-definite for any m x's <br>
    - Note that, the corresponding `phi` may be a infinite-D vector <br>

## Regularization and the non-separable case

17. The assumed that the data is linearly separable may not clearly true <br>
The action of separating may not exactly what we want to do <br>
18. In order to make the algorithm to **work for non-linearly saparable datasets and less sensitive to outliers**: Reformulate the optimization (using l_1 **regularization**) <br>
`min_(xxi,w,b)(0.5 * ||w||^2 + C * sum(xxi_i)) s.t. yi * (w^T * xi + b) >= 1 - xxi_i, i = 1, ..., m; xxi_i >= 0, i = 1, ..., m` <br>
Understand: allow some point to have margin less than `1` by `xxi_i`, but it should pay the cost of `C * xxi_i` <br>
Then the dual optimization problem is `max_alpha(W(alpha)) s.t. 0 <= alpha_i <= C, i = 1, ..., m; sum(alpha_i * yi) = 0` with the same `W(alpha)` <br>
<br>
Moreover, the KKT condition become: <br>
    - `alpha_i = 0` then `yi * (w^T * xi + b) >= 1` <br>
    - `alpha_i = C` then `yi * (w^T * xi + b) <= 1` <br>
    - `0 < alpha_i < C` then `yi * (w^T * xi + b) = 1` <br>

## The SMO Algorithm: To solve dual problem
SMO: Sequential Minimal Optimization <br>

19. Prviously introduced method to solve a max/min problem: Gradient ascent/descent, Newton's method <br>
20. Digression: Coordinate Ascent Algorithm to solve max problem (or min) <br>
```
Loop until convergence:{
    For i = 1, ..., m:{
        alpha_i := arg max W(alpha_1, ...,alpha_m).
    }
}
```
21. SMO <br>
    - Rewrite the problem: `max_alpha(W(alpha)) s.t. 0 <= alpha_i <= C, i = 1, ..., m; sum(alpha_i * yi) = 0` <br>
    - Because the constrain 2, any single `alpha` is determined by others <br>
    - So we need to update at least two of them simultaneously <br>
    ```
    Repeat until convergence:{
        1. Select some pair alpha_i and alpha_j to update next.
        (using heuristic to pick will allow us to make the biggest progress towards the global maximum.)
        2. Reoptimize W(alpha) with respect to alpha_i and alpha_j while holding all the other alpha_k's (k != i, j) fix.
    }
    ```
    - To test for convergence of this algorithm, we can check whether the KKT conditions are satisfied to within some tolerance (typically sat to 0.01 to 0.0001) <br>
    - Can do well because the high efficient in each reoptimization <br>
    - To Exploir: Heurixtics, How to update b <br>

<br>
<br>

# Learning Theory 

## Bias/Variance Tradeoff

1. Generalization error:its expected error on examples not necessarily in the training set <br>
2. Bias `E[f_true(x) - f(x)]` : the expected generalization error even if we were to fit it to a very (say, infinitely) large training set --- Underfit if large <br>
Variance `Var[f(x)]` : --- Overfit if large <br>
3. The total Mean Square Error (MSE): `MSE = E[(y - f(x))^2] = sigma^2 + (Bias[f(x)])^2 + Var[f(x)]` for `y = f_true(x) + e` where `E[e] = 0; Var(e) = sigma^2` <br>

## Learning Theory (Take binary classification as an example)

4. Two Lammas: <br>
    - The union bound lamma; <br>
    - The Hoeffding inequality (Chernoff bound) lamma <br>
5. PAC assumption 1: Training examples are drawn iid (independent and identically distributed) from some prob distribution `hD` <br>
6. Training error (also called Empirical risk or Empirical error): `epsilon'(h) = (1 / m) * sum(1{h(xi) != yi})` <br>
Generalization error: `epsilon(h) = P_((x,y)~hD)(h(x) != y)` (PAC assumption 2) <br>
7. Empirical risk minimization (ERM): a way to fit `theta`: `theta' = argminepsilon'(h_theta)` <br>
Def Hypothesis class `hH` as the set of all classifiers with some constrain, such as linear regression `hH`. And then the argmin domain can be defined by `hH` <br>
8. The case of finite `hH` <br>
    -  `epsilon'(h)` is a reliable estimate of `epsilon(h)` for all `h in hH` <br>
    This implies an upper-bound on the generalization error of `h'` <br>
    - Theorem. (If uniform convergence occurs) Let `|hH| = k`, and let any `m, delta` be fixed, then with prob at least `1 - delta`, we have that <br>
    `epsilon(h') <= minepsilon(h) + 2 * sqr_root(log(2 * k / delta) / (2 * m))` <br>
    - The two part of this equation corresponding to the tradeoff between bias and variance <br>
    - Sample complexity Corollary: Let `|hH| = k`, and let `delta, gamma` be fixed. Then for `epsilong(h') <= minepsilon(h) + 2 * gamma` to hold with prob ar least `1 - delta`, it suffices that <br>
    `m >= log(2 * k / delta) / (2 * gamma^2) = O(...)` <br>
9. The case of infinite `hH` <br>
    - we say that `hH` shatters `S` if `hH` can realize any labeling on `S` <br>
    - Def Vapnik-Chervonenkis dimension of `hH`, written `VC(hH)`, to be the size of the largest set that is shattered by H. (If `hH` can shatter arbitrarily large sets, then `VC(hH) = infinity`) <br>
    - **Vapnik Theorem: (some believe its the most important theorm)**: Let `hH` be given, and let `d = VC(hH)`. Then with prob at least `1 - delta`, we have that for all `h belongs to hH`: <br>
    `|epsilon(h) - epsilon'(h)| <= O(sqr_root(d * log(m / d) / m + log(1 / delta) / m))` also <br>
    `epsilon(h') <= epsilon(h^*) + O(sqr_root(d * log(m / d) / m + log(1 / delta) / m))` <br>
    - Corollary: For `|epsilon(h) − epsilon'(h)| ≤ gamma` to hold for all `h ∈ H` with probability at least `1 − delta`, it suffices that `m = O_(gamma, delta)(d)` <br>
    - Also, `d` is also roughly linear in the number of parameters <br>

## Error Analysis

10. How to make sure that which part of the algorithm have the biggest problem <br>
    - Plug in the ground-truth (that is manually do the work or by some ways to make sure that the work is done perfectly) for each component and see the accuracy increase <br>

## Ablative Analysis

11. How much did each of the improvment really help <br>
    -  start from the current level of performance, and slowly take away all of these features to see how it affects performance <br>

## Mistakes Analysis

12. Analyze what the mistakes composed of, and make improvement according to it <br>

<br>
<br>

# Regularization and Model Selection

## Cross Validation --- The method for Model Selection

1. Hold-out (or called Simple) Cross Calidation: <br>
    - Randomly split S into S_train and S_CV (called the hold-out cross calidation set) <br>
    - Train each model M_i on S_trian only, to get some hypothesis h_i <br>
    - Select and output the hypohesis h_i that had the smallest error `epsilon'_(S_CV)(h_i)` on the hold out cross validation set <br>
    - Optionally, step 3 in the algorithm may also be replaced with selecting the model M_i and then retraining M_i on the entire training set S <br>
2. k-Fold Cross Validation <br>
    - Randomly split s into k disjoint subsets of `m / k` training examples each <br>
    - Use one as validation set and others as training set each loop, and calc the mean `epsilon'_(S_j)(h_i)` for M_i <br>
    - Pick the M_i corresponding to the smallest mean <br>
    - Usually `k = 10`; <br>
    Extramly situation: `k = m` --- Leave-One-Out Cross Validation <br>
3. These also can be used more simply to evaluate a single model or algorithm <br>

## Feature Selection --- A special case of Model Selection

4. For the case that number of features is too large that will easily cause overfitting <br>
5. Total number of models is `2^n` which is too large --- need a kind of heuristic search procedure <br>
<br>
6. Forward Search (a little bit like greedy algorithm) <br>
    - Initialize `hF = ∅` <br>
    - Repeat till some pre-set threshold{
        - For i = 1, ..., n if `i doesnt belong to hF` let `hF_i = hF U {i}` , and use some version of cross validation to evaluate features `hF_i` <br>
        - Set `hF` to be the best feature subset found on the above step <br>
    }
    - Select and output the best feature subset that was evaluated during the entire search procudure <br>
7. Wrapper Model Feature Selection <br>
8. Backeward Search <br>
<br>
9. Filter Feature Selection: much cheaper <br>
The idea here is to compute some simple score S(i) that measures how informative each feature x_i is about the class labels y. Then, we simply pick the k features with the largest scores S(i) <br>
10. One possible choice of the score would be define S(i) to be the correlation between x_i and y, as measured on the training data <br>
11. Mutual Information `MI(x_i, y)`: <br>
    - `MI(x_i, y) = sum_(x_i)(sum_y(p(x_i, y) * log(p(x_i, y) / (p(x_i) * p(y)))))` <br>
    - `p`'s can all be estimated according to their empirical distributions on the training set <br>
    - Expressed as a Kullback-Leibler (KL) divergence: <br>
    `MI(x_i, y) = KL(p(x_i, y) || p(x_i) * p(y))` <br>

## Bayesian Statistics and Regularization

12. Frequentist: Not random just happen to be unknown; <br>
Bayesian: Random and unknown <br>
13. Prior Distribution: `p(theta)` <br>
`p(theta | S) = p(S | theta) * p(theta) / p(S) = pi(p(yi | xi, theta)) * p(theta) / \jf\_theta(pi(p(yi | xi, theta)) * p(theta))dtheta` <br>
Posterior Distribution: `p(y | x, S)` <br>
`p(y | x, S) = \jf\_theta(p(y |x, theta) * p(theta | S))dtheta` <br>
14. MAP (maximum a posteriori) estimate for theta: <br>
`theta_MAP = argmax(pi(p(yi | xi, theta) * p(theta)))` <br>
15. Similar to the ML (maximum liklihood) estimate for theta in Frequentist: <br>
`theta_ML = argmax(pi(p(yi | xi, theta)))` <br>
16. In prectice, `p(theta)` is to assume that `theta ~ N(0, tao^2 * I)` <br>
Use this, MAP can get a `theta` with smaller norm than ML and is less susceptible to overfitting than ML <br>

<br>
<br>

# Decision Tree & Ensembling Methods

## Decision Tree: a Non-linearity, region-based algorithm

1. Def of Non-linearity <br>
A simple example of it is that `Lattitude & Mouth => Can Ski or not` --- Select Region which is intractable
2. Decision Tree: An approximate solution via **greedy, top-down, recursive partitioning** <br>
    - Top-down: We start with the original input space `Xxi` and split it into two child regions by thresholding on a single feature <br>
    - Recursive: We then take one of these child regions and can partition via a new threshold, and continue to train our model in a recursive manner <br>
    (always selecting a leaf node, a feature, and a threshold to form a new split) <br>
    - We can continue in such a manner until we a meet a given stop criterion, and then predict the majority class at each leaf node <br>
<br>
3. The Loss Function of Decision Tree <br>
    - When the parent region `R_p` ge splited into two child regions `R_1, R_2`, then the decrease in loss is: <br>
    `L(R_p) - (|R_1| * L(R_1) + |R_2| * L(R_2)) / (|R_1| + |R_2|)` (cardinality-weighted sum) <br>
4. A straight-forward `L` that we can think about: Misclassification Loss `L_misclass` <br>
    - `L_misclass(R) = 1 - max(p'_c)` <br>
    - `p'_c` is the prob of misclassification if we denote all the element in the region as `c` <br>
    - But it is not sensitive enough, sometimes: not only are the losses of the two splits identical, but neither of the splits decrease the loss over that of the parent <br>
5. A more sensitive loss: Cross-Entropy `L_cross` <br>
    - `L_cross(R) = - sum_c(p'_c * log(p'_c))` <br>
    - From an information-theoretic perspective, cross-entropy measure the number of bits needed to specify the outcome (or class) given that the distribution is known <br>
    - Furthermore, the reduction in loss from parent to child is known as information gain <br>
6. The regression setting for Decision Trees <br>
    - To predict, instead of using the major value of `y`'s in the region `R`, we use the mean: `y' = sum_(i in R)(y_i) / |R|`
    - Then we can directly use the squared loss `L_squared`: <br>
    `L_squared(R) = sum_(i in R)(y_i - y')^2 / |R|` <br>
<br>
7. A unique advantage: Can deal with categorical variables <br>
8. Regularization or Stopping Criteria <br>
    - Minimum Leaf Size <br>
    - Maximum Depth <br>
    - Maximum Number of Nodes <br>
    - Minimum decrease in loss after split is not good, because it means missing higher order interactions (like needing thresholding on multiple features to achieve a good split) <br>
    - Fully growing out the tree, and then pruning away nodes that minimally decrease misclassification or squared error, as measured on a validation set <br>
9. Runtime <br>
10. Disadvantages: <br>
    - High variance (use ensembling which will introduce nxet to solve it) <br>
    - Feature-based, so that can not rotate the picture, which means it can just use - & | to approximately modeled a \ <br>

## Ensembling Methods

1. By bias/variance analysis <br>
    - Ensembling will lead to a decrease in variance of the error <br>
    `Var(X_mean) = rou * sigma^2 + (1 - rou) * sigma^2 / n` <br>
    where `rou` is a measurement of correlation between each model <br>
    - Both increasing the number of models used to ensembling and decreasing the correlation between models will cause to an over all decrease in variance of the error of the ensenmble <br>
2. Ways to generate de-correlated models <br>
    - Using different algorithms <br>
    - Using different training sets <br>
    - Bagging <br>
    - Boosting <br>
<br>
3. Bagging: 'Bootstrap Aggregation'
    - Bootstrap: <br>
    True population P, A training set S sampled from P ( `S ~ P` ) <br>
    we can generate many new bootstrap sets Z_i (i = 1, ..., n) sampled with replacement from S ( `Z ∼ S, |Z| = |S|` ) **???** <br>
    We can then look at the variability of our estimate across these bootstrap sets to obtain a measure of error
    - Aggregation: <br>
    we can take each Z_i and train a machine learning model G_i on each, and define a new aggregate predictor: `G(X) = sum_i(G_i(x) / n)` <br>
    <br>
    - Decreasing variance by decreasing `rou` compare to all just simply trained on S <br>
    - Note that `rou` is indepent upon `n` , by increasing `n`, the variance only decrease <br>
    <br>
    - Out-of-bag estimation: <br>
        -  each bootstrapped sample only contains approximately 2/3 of S, and thus we can use the other 1/3 as an estimate of error, called out-of-bag error <br>
        - In the limit, as `n -> infinity`, out-of-bag error gives an equivalent result to leave-one-out cross-validation <br>
    <br>
    - Use upon Dicision tree: <br>
        - Work well <br>
        - Support for missing values <br>
        - Lose interprtability <br>
        - Varaible importance measure <br>
        - Random forests: only allow a subset of features to be used at each split, to prevent a very strong predictor (feature) causing large `rou` <br>
<br>
4. Boosting: use for bias-reduction <br>
    - Get high bias, low variance models (weak learners) by only allow Decision Trees split once (Decision Stumps): <br>
    The key idea is that after one training, we then track which examples the classifier got wrong, and increase their relative weight compared to the correctly classified examples, and then do the next training <br>
    <br>
    - Adaboost <br>
        - Pseudocode (N sample data) <br>
        ```pascal
        w_i := 1 / N, for i = 1, 2, ..., N
        for m = 0 to M do
            Fit weak classifier G_m to training data weighted by w_i
            Compute weighted error err_m = sum_i(w_i * 1{y_i != G_m(x_i)}) / sum(w_i)
            Compute weight alpha_m = log((1-err_m) / err_m)
            w_i := w_i * exp(alpha_m * 1{y_i != G_m(x_i)})
        end
        f(x) = sign(sum_m(alpha_m * G_m(x)))
        ```
        - The G's are not independent, meaning that increasing M leads to an increase in the risk of overfitting <br>
    - Forward Stagewise Additive Modeling
        - Pseudocode <br>
        ```pascal
        Initialize f_0(x) = 0
        for m = 0 to M do
            Compute (beta_m, gamma_m) = argmin(sum_i(L(y_i, f_(m-1)(x_i) + beta * G(x_i; gamma))))
            Set f_m(x) = f_(m-1)(x) + beta_m * G(x; y_i)
        end
        f(x) = f_M(x)
        ```
        - Intuition: Every step tries to find the weak learner that fit the rest best **???** <br>
        - Adaboost is a special case of it <br>
    - Xgboost
    - Gradient Boosting: Use gradient descent to solve the min problem. But we are restricted to taking steps in our model class <br>
        - We instead compute the gradient at each training point with respect to the current predictor <br>
        `g_i = pdL(y, f(x_i)) / pdf(x_i)`
        - Then train a new regression predictor to match this gradient and use it as the gradient step <br>
        In Forward Stagewise Additive Modeling `gamma_i = argmin(sum_i(g_i - G(x_i; gamma)))` <br>

<br>
<br>

# Deep Learning

## Neural Networks

1.  Introduce to building of Neural Networks: <br>
    - Introduce to ReLU <br>
    - Introduce to feature generating <br>
    - Introduce to the stacking <br>
    - End-to-End Learning <br>
2. Notations: <br>
    - Inputs: xi --- i-th input; x_i = a0_i; <br>
    - Hidding layers outputs: a[l]_j --- j-th unit in layer l <br>
    - foo[i] meanings anything associated with layer i <br>
    - w --- weight vectors; W --- weight matrix; W_i --- the i-th row of W <br>
3. A neural is consist of linear regression and non-linear activation <br>
`z[l]_i = W[l]_i^T * a[l-1] + b[l]_i` (where a0 = x) <br>
`a[l]_i = g(z[l]_i)` <br>

## Vectorization

4. Calc with vectorization <br>
`z[l] = W[l] * x + b[l]` and `a[l] = g(z[l])` <br>
5. The necessity of using non-linearity <br>
6. Moreover `Z[l] = W[l] * X + (broadcast)b[l]` <br>

## Backpropagation

7. Parameter Initialization <br>
    - Property Symmetry <br>
    - Solution: <br>
        - Random Initialization <br>
        - Xavier/He Initialization <br>
        Donote `n[l]` as the number of neurons in layer `l` , then `w[l] ~ hN(0, sqr_root(2 / (n[l] + n[l + 1])))` <br>
8. Optimization <br>
    - Backpropagation <br>
    - Mini-batch Gradient Descent <br>
    Momentum <br>
9. Regularization <br>
    - L2 Regularization <br>
    `J_L2 = J + lamda * ||W||^2 / 2` <br>
    - Parameter Sharing: Convolutional Neural Networks <br>

<br>
<br>

# Unsupervised Learning Problem

## k-Means clustering algorithm

1. we are given a training set and want to group the data into a few cohesive “clusters”. Which means no labels yi are given <br>
2. The k-mean clustering algorithm <br>
    ```
    1. Initialize cluster centroids miu_i, i=1, ..., k randomly
    2. Repeat until convergence: {
        For every i, set:  ci := argmin_j(||xi - miu_j||^2)
        For every j, set:  miu_j := sum_i(1{ci=j} * xi) / sum_i(1{ci=j})
    }
    ```
3. Distortion Function <br>
    - `J(c,miu) = sum_i(||xi - miu_ci||^2)` <br>
    - What we are doing is repeatedly minimizing `miu` and `c` while holding another fixed <br>
    - But `J` is a non-convex function, so may get stuck in a locl minima <br>
    - We can run it many times with different initializing and use the one that gives the lowest distortion <br>

## Mixtures of Gaussians and the EM algorithm

1. Mixture of Gussians: <br>
What we want is to specifying a joint distribution `p(xi, zi) = p(xi | zi) * p(zi)` <br>
Where `xi`'s is the input training set, `zi ~ Multinomial(phi)` is a latent variable and `xi | zi = j ~ hN(miu_j, Sigma_j)`, let `k` denote the number of values that the `zi`'s can take on <br>
2. Likelihood: <br>
`l(phi, miu, Sigma) = sum_i(log(p(xi; phi, miu, Sigma))) = sum_i(log(sum_zi(p(xi | zi; miu, Sigma) * p(zi, phi))))` <br>
3. `zi`'s are unknown, so it's difficult to solve. But if `zi`'s are known, then this become a classification problem, and easy to solve. <br>
    - `phi_j = sum_i(1{zi = j}) / m` (m, the number of inputs) <br>
    - `miu_j = `sum_i(1{zi = j} * xi) / sum_i(z{zi = j}) <br>
    - `Sigma_j = sum_i(1{zi = j} * ||xi - miu_j||^2) / sum_i(1{zi = j})` <br> 
<br>
4. The EM Algorithm:
    - E-step: try to guess the calues of the `zi`'s <br>
    M- step: Updates the parameters of out model based on out guesses <br>
        ```
        Repeat until convergence: {
            (E) For each `i, j`, set
                wi_j := p(zi = j | xi; phi, miu, Sigma)
            (M) Update the parameters by 
                phi_j := sum_i(wi_j) / m
                miu_j := sum_i(wi_j * xi) / sum_i(wi_j)
                Sigma := sum_i(wi_j * ||xi - miu_j||^2) / sum_i(wi_j)
        }
        ```
    Where `p(zi = j | xi; phi, miu, Sigma) = p(xi | zi = j; miu, Sigma) * p(zi = j; phi) / sum_l(p(xi | zi = l; miu, Sigma) * p(zi = l; phi))` <br>
    - Compared with k-mean, <br>
        - k-means use the “hard” cluster assignments `ci` <br>
        - EM use the “soft” assignments `wi_j` <br>
        - Similar to K-means, it is also susceptible to local optima, so reinitializing at several different initial parameters may be a good idea <br>

<br>
<br>

# The EM Algorithm: A More General View

## Jensen's Inequality

1. If `f''(x) > 0` for all x (in the vector-valued case, the corresponding statement is that `H` must be positive definite, written `H > 0`), then we say f is strictly convex <br>
<br>
2. Jensen's Inequality Theorem: <br>
let `f` be a convex function, and let X be a random variable. Then: <br>
`E[f(X)] >= f(E[X])` <br>
Moreover, if `f` is strictly convex, then `=` holds true if and only if `X = E[X]` with probability 1 (i.e., if X is a constant) <br>
<br>
3. Concave virsion of Jensne's Inequality <br>

## The EM Algorithm

4. Inputs training set `{x1, ..., xm}`, we wish to fit the parameters of a model `p(x, z)` to the data, where the likelihood is given by: <br>
`l(theta) = sum_i(log(p(x; theta))) = sum_i(log(sum_z(p(x, z; theta))))` <br>
Where `zi`'s are the latent random variables <br>
<br>
5. The EM algorithm: <br>
    - Idea: As explicitly maximizing `l(theta)` might be difficult, we instead repeatedly construct a lower-bound on `l` (E-step), and then optimize that lower-bound (M-step) <br>
    - For each i, let `Q_i` be some distribution over the `z`'s (`sum_z(Q_i(z)) = 1, Q_i(z) >= 0`). Then <br>
    `sum_i(log(p(xi; theta))) =` <br>
    `sum_i(log(sum_zi(p(xi, zi; theta)))) =` <br>
    `sum_i(log(sum_zi(Q_i(zi) * p(xi, zi; theta) / Q_i(zi)))) >=` <br>
    `sum_i(sum_zi(Q_i(zi) * log(p(xi, zi; theta) / Q_i(zi))))` <br>
    Which give a lower bound for any distribution `Q_i` <br>
    - For any fix theta, we'll try to make the bound tight, which is to say: let the equality hold: <br>
    `p(xi, zi; theta) / Q_i(zi) = c` <br>
    Also `sum_z(Q_i(z)) = 1` <br>
    We have `Q_i(zi) = p(xi, zi; theta) / sum_z(p(xi, z; theta)) = p(xi, zi; theta) / p(xi; theta) = p(zi | xi; theta)` <br>
    - Above gives us the EM algorithm <br>
        ```
        Repeat until convergence: {
            (E) For each i, set
                Q_i(zi) := p(zi | xi; theta)
            (M) Set
                theta := argmax(sum_i(sum_zi(Q_i(zi) * log(p(xi, zi; theta) / Q_i(zi)))))
        }
        ```
    - The prove of the algorithm will convege, and  one reasonable convergence test would be to check if the increase in `l(θ)` between successive iterations is smaller than some tolerance parameter, and to declare convergence if EM is improving `l(θ)` too slowly <br>

## Specified with the Example Before: Mixture of Gaussians

<br>
<br>

# Factor Analysis

1. Unless m exceeds n by some reasonable amount, the maximum likelihood estimates of the mean and covariance may be quite poor <br>

## Restrictions of Sigma

2. If we do not have sufficient data to fit a full covariance matrix, we may place some restrictions on the space of matrices Σ that we will consider <br>
<br>
3. Fit a covariance matrix Σ that is diagonal: (`m` is the number of x) <br>
`Sigma_jj = sum_i((xi_j - miu_j)^2) / m` <br>
A diagonal Sigma corresponds to a Gaussian where the major axes of these ellipses are axis-aligned <br>
4. Sometimes, we may place a further restriction on the covariance matri that not only must it be diagonal, but its diagonal entries must all be equal (`m` is the number of x, `n` is the number of dimentions) <br>
`Sigma = sigma^2 * I; sigma^2 = sum_j(sum_i((xi_j - miu_j)^2)) / (m * n)` <br>
This model corresponds to using Gaussians whose densities have contours that are hyperspheres <br>
<br>
5. Restricting Σ to be diagonal also means modeling the different coordinates x_i, x_j of the data as being uncorrelated and independent <br>
Therefore, it'll fail to captures any correlations in the data <br>

## Marginals and Conditionals of Gaussians

6. 分块(Marginal)向量化高斯分布: <br>
    - `x=(x_1, ..., x_n)^T` , `miu=(miu_1,..., miu_n)^T` <br>
    `Sigma = (Sigma_ij)_n*n`, where `Sigma_ij = E[(x_i - miu_i) * (x_j - miu_j)^T]` <br>
    - Marginal distributions of Gaussians are themselves Gaussian: `x_i ~ hN(miu_i, Sigma_ii)` <br>
    - The conditional distribution of x_i given x_j: `x_i | x_j ~hN(miu_i|j, Sigma_i|j)` , where `miu_i|j = miu_i + Sigma_ij * (Sigma_jj)^-1 * (x_j - miu_j)` ; `Sigma_i|j = Sigma_ii - Sigma_ij * (Sigma_jj)^-1 * (Sigma_ji)` <br>

## The Factor Analysis Model

7. We posit a jointt distribution on (x, z) as follows, where z (k-D) is a latent random variable: (x is n-D, and usually k < n) <br>
`z ~ hN(0, I)` ; `x|z ~ hN(miu + Lamda * z, Psi)` (`Psi` is a diagonal matrix) <br>
8. Or equalvalence: <br>
`z ~ hN(0, I)` ; `epsilon ~ hN(0, Psi)` ; `x = miu + Lamda * z + epsilon` <br>
<br>
9. Let's work out exactly what distribution our model defines: <br>
    - First, there's a joint distribution: <br>
    `(z, x)^T ~ hN(miu_zx, Sigma)` <br>
    - Find `miu_zx`: <br>
    We have: `E[z] = E[epsilon] = 0` <br>
    Then `E[x] = miu + Lamda * E[z] + E[epsilon] = miu` <br>
    So `miu_zx = (0(k-D), miu)^T` <br>
    - Find `Sigma`: <br>
    `Sigma_zz = I` <br>
    `Sigma_zx = E[(z - E[z]) * (x - E[x])^T] = E[z * (miu + Lamda * z + epsilon - miu)^T] = E[z * z^T] * Lamda^T + E[z * epsilon^T] = Lamda^T` <br>
    `Sigma_xz = Lamda` <br>
    `Sigma_xx = Lamda * Lamda^T + Psi` <br>
    - Maximize likelihood: `x ~ hN(miu, Lamda * Lamda^T + Psi)` <br>

## EM for Factor Analysis

10. Using former equition to calc `miu_zi|xi; Sigma_zi|xi` <br>
Then generate the expression of `Q_i(zi)` <br>
11. Then we can get how the optimization is done in M-step <br>
12. Note that `E[z * z^T]` and `E[z] * E[z^T]` is different <br>

<br>
<br>

# Principal Components Analysis (PCA)

1. For a dataset `xi, i = 1, ..., m`, where `xi` is n-d (`n<<m` and unknown to us). <br>
Which means the dimension now we know for `x` has many redundancy. I.e. from some of the featrues we can derive the other some of them <br>
Which can be seen as the input approximately lie on some hyperline or direction or diagonal axis in the high-d space <br>
<br>
2. First we do the centering (zero-meaning) and normalizing of the input `xi`'s <br>
Centering can be omitted if the input is already zero mean; Normalizing can be omitted if the different attributes are all on the same scale <br>
3. Then to find the 'axis', one way to do so is as finding the unit vector `u` ( `|u| = 1` ) so that when the data is projected onto the direction corresponding to `u`, the variance of the projected data is maximized <br>
`u = argmax(sum_i((xi^T * u)^2) / m) = argmax(u^T * (sum_i(xi * xi^T) / m) * u)` <br>
    - Where as we zero-meaned the input `x` , we have `Sigma = sum_i(xi * xi^T) / m` is just the covariance matrix of the data <br>
    - We easily recognize that what we want to do here is to find the principal eigenvector of `Sigma` <br>
4. More generally, if we wish to project our data into a k-dimensional subspace ( `k < n` ), we should choose `u_1, ..., u_k` to be the top `k` eigenvectors of `Sigma` <br>
And the `u_i`'s now form a new, orthogonal basis for the data <br>
5. Then to represent `xi` in this basis, we need only compute the corresponding vector `yi` : <br>
`yi = (u_1^T * xi, ..., u_k^T * xi)^T` <br>
<br>
6. PAC is also called Dimensionality Reduction algorithm. <br>
The vectors `u_i`'s are called the first `k` principal components of the data <br>
<br>
7. Application: <br>
    - Compression <br>
    - Plot and view <br>
    - Reduce demension before training <br>
    - Noise reduction algorithm <br>

# Independent Components Analysis (ICA)

1. Some data s (n dimension) that is generated via n independent sources <br>
What we observe is `x = A * s` where A is an unknown square matrix called the mixing matrix <br>
The dataset { `xi`; `i = 1,...,m` }, and out goal is to recover the sources `si` that had generated out data ( `xi = A * si` ) <br>
Let `W = A^-1` be the unmixing matrix, our goal is to find `W` (and let `w_i^T` denote the i-th row of `W`) <br>

## ICA Ambiguities

2. There are some inherent ambiguities in `A` that are impossible to recover, given only the `xi`'s <br>
    - Permutation <br>
    - Scaling <br>
    - For a Gaussian distribution `s` , because of the circle shape of Gaussian distribution, there will be an arbitrary retatoinal component in the mixingg matrix that cannot be determined from the date <br>
3. As long as the data is not Gaussian, it is possible, given enough data, to recover the n independent sources <br>

## Densities and linear transpormations

4. Let `p_s(s)` be the density of `s` , and `p_x(x)` be the density of `x` , and other notation are inherented. Then we have: <br>
`p_x(x) = p_s(W * x) * |W|` <br>
Where `|W|` is the determinant of the matrix `W` <br>

## ICA Algorithm

5. The derivation of ICA: <br>
    - We suppose that the distribution of each source `s_i` is given by a density `p_s`, and that the joint distribution of the sources `s` is given by <br>
    `p(s) = pi_i(p_s(s_i))` <br>
    Note that by modeling the joint distribution as a product of the marginal, we capture the assumption that the sources are independent <br>
    - Then we get: <br>
    `p(x) = pi_i(p_s(w_i^T * si)) * |W|` <br>
    And all we need to do is to specify a density for the individual sources p_s <br>
    - To do so, all we need to do is to specify some cumulative distribution function (cdf) for it <br>
    As we cannot choose the cdf of Gaussian distribution, we choose sigmoid function <br>
    Then `p_s(s) = g'(s)` <br>
    - The likelihood: <br>
    `hl(W) = sum_i(sum_j(log(g'(w_j^T * xi))) + log(|W|))` <br>
    - Knowing that `Delta_W(|W|) = |W| * W^-1^T` <br>
    We can derive that `W := W + alpha * ([1 - 2 * g(w_i^T * xi)]_n*1 * xi^T + (W^T)^-1)` <br>
    Where `alpha` is the learning rate <br>

<br>
<br>

# Reinforcement Learning and Control

## Markov Decision Process (MDP)

1. A MDP is a tuple `(S, A, {P_sa}, gamma, R)` where <br>
    - `S` is a set of states <br>
    - `A` is a set of actions <br>
    - `P_sa` are the state transition probabilities, gives the distribution over what states we will transition to if we take action `a` in state `s` <br>
    - `gamma` belongs to `[0,1)` is called discount factor <br>
    - `R: S (x A) >-> R` is the reward function <br>
<br>
2. The dynamics of an MDP proceeds as follows:<br>
We start in some state `s_0`, and get to choose some action `a_0 ∈ A` to take in the MDP. As a result of our choice, the state of the MDP randomly transitions to some successor state `s_1`, drawn according to `s_1 ∼ P_s0a0`. Then, we get to pick another action `a_1` ...... <br>
3. Upon visiting the sequence of states `s_0, s_1, ...` with actions `a_0, a_1, ...` , our
total payoff is given by <br>
`R(s_0, a_0) + gamma * R(s_1, a_1) + gamma^2 * R(s_2, a_2) + ...` <br>
Or `R(s_0) + gamma * R(s_1) + gamma^2 * R(s_2) + ...` (take this as example below for simplity) <br>
4. Our goal in reinforcement learning is to choose actions over time so as to maximize the expected value of the total payof <br>
`E[R(s_0) + gamma * R(s_1) + gamma^2 * R(s_2) + ...]` <br>
<br>
5. Def: Policy <br>
A policy is any function `pi: S >-> A` mapping from the states to the actions <br>
6. Def: Value function <br>
The value function for a policy `pi` is <br>
`Vpi(s) = E[R(s_0) + gamma * R(s_1) + gamma^2 * R(s_2) + ... | s_0 = s, pi]` <br>
7. Def: Optimal Value function <br>
`V^*(s) = max_pi(Vpi(s))` <br>
<br>
8. Bellman equations: (The value function satisfies) <br>
    - `Vpi(s) = R(s) + gamma * sum_s'(P_(spi(s))(s') * Vpi(s')) = R(s) + gamma * E_(s'~P_spi(s))[Vpi(s')]` <br>
    - **Note**: Bellman’s equations can be used to efficiently solve for `Vpi`. Specifically, in a finite-state MDP, we can write down one such equation for `Vpi(s)` for every state `s`. This gives us a set of `|S|` linear equations in `|S|` variables  which can be efficiently solved <br> 
    - Bellman equation for the optimal value function: <br>
    `V^*(s) = R(s) + max_a(gamma * sum_s'(P_sa(s') * V^*(s')))` <br>
9. Def: `pi^*` (Equation (3)) <br>
`pi^*(s) = argmax_a(sum_s'(P_sa(s') * V^*(s')))` <br>
10. Then we get for every state `s` and every policy `pi` : <br>
`V^*(s) = Vpi^*(s) >= Vpi(s)` <br>
Note that `pi^*` has the interesting property that it is the optimal policy for all states `s` <br>

## Value iteration and policy iteration
For now, we will consider only MDPs with finite state and action spaces <br>

11. The Value Iteration <br>
    - the process <br>
        ```
        1. For each state s, initialize V(s) := 0.
        2. Repeat until convergence: {
            For every state, update V(s) := R(s) + max_a(gamma * sum_s'(P_sa(s') * V(s')))
        }
        ```
    - Synchronous and Asynchronous updates <br>
    - Having found `V^*` , we can then use Equation (3) to find the optimal policy <br>
<br>
12. The Policy Iteration <br>
    - The process <br>
        ```
        1. Initialize pi randomly.
        2. Repeat until convergence: {
            (a) Let V := Vpi
            (b) For each state s, let pi(s) := argmax_a(sum_s'(P_sa(s') * V(s')))
        }
        ```
    - Note that step (a) can be done via solving Bellman’s equations as described earlier <br>

## Learning a model for an MDP

13. In many realistic problems, we are not given state transition probabilities and rewards explicitly, but must instead estimate them from data <br>
<br>
14. Given that the MDP consisting of a number of trials, we can then easily derive the maximum likelihood estimates for the state transition probabilities <br>
`P_sa(s') = No. of times took we action a in state s and got to s' / No. of times we took action a in state s` <br>
Or if the ratio above is `0 / 0` , we maight simply estimate `P_sa(s')` to be `1 / |S|` <br>
15.  if `R` is unknown, we can also pick our estimate of the expected immediate reward `R(s)` in state `s` to be the average reward
observed in state `s` <br>

## Continuous state MDPs
We now discuss algorithms for MDPs that may have an infinite number of states <br>

16. Discretization <br>
Downsides: <br>
    - it assumes that the value function is takes a constant value over each of the discretization intervals <br>
    - curse of dimensionality <br>
<br>
17. Value function approximation: Using a model or simulator <br>
    - a simulator is a black-box that takes as input any (continuous-valued) state `s_t` and action `a_t`, and outputs a next-state `s_(t+1)` sampled according to the state transition probabilities `P_stat` <br>
    - Several ways to get such model: <br>
        - Physics simulation <br>
        - Get a model is to learn one from data collected in the MDP <br>
        Deterministic / Stochastic model <br>
18. Value function approximation: Fitted value iteration algorithm <br>
    - Assume that the problem has a continuous state space `S` of `n-D`, but that the action space `A` is small and discrete <br>
    - Recall that in value iteration <br>
    `Vpi(s) = R(s) + gamma * \jf\_s'(P_(spi(s))(s') * Vpi(s'))ds' = R(s) + gamma * E_(s'~P_spi(s))[Vpi(s')]` <br>
    - The main idea: <br>
        - Approximately carry out this step, over a finite sample of states `si` using machine learning <br>
        - Specifically, we will use a supervised learning algorithm—linear regression in our description below—to approximate the value function as a linear or non-linear function of the states: <br>
        `V(s) = theta^T * phi(s)` , where `phi` is some appropriate feature mapping of the states <br>
    - For each state `s` in our finite sample of `m` states, fitted value iteration will first compute a quantity `yi` , which will be our approximation to `R(s) + gamma * max_a(E_(s'∼P_sa)[V(s')])` . Then, it will apply a supervised learning algorithm to try to get `V(si) = theta^T * phi(si)` close to `yi` <br>
    - Cannot be proved to always converge <br>
    - When we need to choose an action for some state `s`, The process for computing/approximating this is similar to the inner-loop of fitted value iteration <br>
    - The method to choose the `k` above <br>

<br>
<br>

# LQR, DDP and LQG
Linear Quadratic Regulation, Differential Dynamic Programming and Linear Quadratic Gaussian <br>

## Finite-horizon MDPs

1. To place ourselves in a more general setting: <br>
    - We rewrite the sum or integal into the expectation form: `E_(s'~P_sa)[Vpi^*(s')]` <br>
    - Assume that `R(s, a)` , then the reward function will go into the max or argmax <br>
    - Finite horizon: we set a finite ending time `T` <br>
    Therefore, our definition of payoff is going to be slightly different: ( `gamma` become not necessary) <br>
    `R(s_0, a_0) + R(s_1, a_1) + ...+ R(s_T, a_T)` <br>
<br>
2. In this new setting, things behave quite differently: <br>
    - First, the optimal policy `pi^*` might be non-stationary, meaning that it changes over time: <br>
    `pit : S -> A` (e.g. `a_0 = pi0(s_0)` ) <br>
    - Time Dependent dynamics: <br>
    `s_(t+1) ~ Pt_stat` <br>
    `Rt(s_t, a_t)` <br>
    Then `(S, A, Pt_sa, T, Rt)` <br>
    - Def Value function and optimal value function as before <br>
<br>
3. Bellman's equation is made for Dynamic Programming <br>
4. Solve for the optimal value function: <br>
    ```
    1. compute V^*_T := max_a(RT(s,a))
    2. for t = T-1, ..., 0:
        compute V^*_t := max_a(Rt(s, a) + E_(s'~Pt_sa)[V^*_(t+1)(s')])
    ```

## Linear Quadratic Regulation (LQR)

5. Assumptions: <br>
    - S n-D, A d-D <br>
    - Linear transitions with noise: <br>
    `s_(t+1) = A_t * s_t + B_t * a_t + w_t` <br>
    where Gaussian noise `w_t ~ hN(0, Sigma_t)` <br>
    - Quadratic Rewards: <br>
    `Rt(s_t, a_t) = -s_t^T * U_t * s_t - a_t^T * W_t * a_t` <br>
    where `U_t, W_t` are both positive definite matrices <br>
    **Note** that the quadratic formulation of the reward is equivalent to saying that we want to take smooth action to make our state be close to the origin <br>
6. Steps: <br>
    - step 1: If we need to estimate `A, B, Sigma`, first, collect transitions from an arbitrary policy. Then, use linear regression to find `argmin_(A, B)(sum_i(sum_t(||si_(t+1) - (A * si_t + B * ai_t)||^2)))` . Finally, use a technique seen in Gaussian Discriminant Analysis to learn Sigma <br>
    - Step 2: Derive the optimal policy using dynamic programming <br>
        - Fact 1: It can be shown that if `V^*_(t+1)` is a quadratic function in `s_t` , then `V^*_t` is also a quadratic function: <br>
        If `V^*_(t+1)(s_(t+1)) = s_(t+1)^T * phi_(t+1) * s_(t+1) + psi_(t+1)` <br>
        Then `V^*_t(s_t) = s_t^T * phi_t * s_t + psi_t` <br>
        Where for `t = T` , we had `phi_t = -U_T, psi_t = 0` <br>
        - Fact 2: We can show that the optimal policy is just a linear function of the state <br>
        Discrete Ricatti equation: <br>
        `phi_t = A_t^T * (phi_(t+1) - phi_(t+1) * B_t * (B_t^T * phi_(t+1) * B_t - W_t)^-1 * B_t * phi_(t+1)) * A_t - U_t` <br>
        `psi_t = -tr(Sigma_t * phi_(t+1)) + psi_(t+1)` <br>
        - Fact 3:  We notice that `phi_t` depends on neither `psi` nor the noise `Sigma`.  It implies that the optimal policy also does not depend on the noise <br>
        ((But `psi_t` does depend on `Sigma_t` , which implies that `V^*_t` depends on `Sigma`) <br>
    -  As the optimal policy does not depend on `psi_t` , and the update of `phi_t` only depends on `phi_t` , it is sufficient to update only `phi_t` <br>

## From non-linear dynamics to LQR

7. It turns out that a lot of problems can be reduced to LQR, even if dynamics are non-linear <br>
8. Taylor expansion: works well for cases where the goal is to stay around some state `s^*` <br>
9. Differential Dynamic Programming (DDP) <br>
We’ll cover a method that applies when our system has to follow some trajectory (think about a rocket). This method is going to discretize the trajectory into discrete time steps, and create intermediary goals around which we will be able to use the previous technique <br>
10. Steps: <br>
    - Step 1: come up with a nominal trajectory using a naive controller, that approximate the trajectory we want to follow. In other words, our controller is able to approximate the gold trajectory with: <br>
    `s^*_0, a^*_0 -> s^*_1, a^*_1 -> ...` <br>
    - Step 2: linearize the dynamics around each trajectory point `s^*_t` using Taylor expansion <br>
    Note We can apply a similar derivation for the reward `Rt` , with a second-order Taylor expansion <br>
    - Step 3: Now, the problem is strictly re-written in the LQR framework. Let’s just use LQR to find the optimal policy `pi_t` . As a result, our new controller will (hopefully) be better <br>
    - Step 4:  We use the new controller (our new policy `pi_t`) to produce a new trajectory <br>
    Note that when we generate this new trajectory, we use the real `F` and not its linear approximation to compute transitions <br>
    - Then, go back to step 2 and repeat until some stopping criterion <br>

## Linear Quadratic Gaussian (LQR)

11. So far, we assumed that the state was available. As this might not hold true for most of the real-world problems, we need a new tool to model this situation: <br>
**Partially Observable MDPs** (POMDP) <br>
12. A POMDP is an MDP with an extra observation layer. <br>
In other words, we introduce a new variable `o_t` , that follows some conditional distribution given the current state `s_t` <br>
`o_t | s_t ~ O(o | s)` <br>
13. Formally, a finite-horizon POMDP is given by a tuple: <br>
`(S, O, A, P_sa, T, R)` <br>
Within this framework, the general strategy is to maintain a belief state (distribution over states) based on the observation `o_1, ..., o_t` . Then, a policy in a POMDP maps belief states to actions <br>
<br>
14. Suppose that, we observe `y_t (m-D)` with `m < n` such that: <br>
`y_t = C * s_t + v_t` <br>
`s_(t+1) = A * s_t +  B * a_t + w_t` <br>
Where `v_t` is the sensor noise (also gaussian, like `w_t` ) <br>
15. Note that the reward function `Rt` is left unchanged, as a function of the state (not the observation) and action <br>
<br>
16. Steps: <br>
    - Step 1: first, compute the distribution on the possible states ( `s_t|t, Sigma_t|t` ), based on the observations we have: <br>
    `s_t | y_1, ..., y_t ~ hN(s_t|t, Sigma_t|t)` <br>
    Using Kalman Filter algorithm <br>
    - Step 2: We’ll use the mean `s_t|t` as the best approximation for `s_t` <br>
    - Step 3: then set the action `a_t := L_t * s_t|t` where `L_t` comes from the regular LQR algorithm <br>
17. Kalman filter algorithm <br>
    - Predict step: compute `s_(t+1) | y_1, ..., y_t` <br>
    Suppose we know: `s_t | y_1, ..., y_t ~ hN(s_t|t, Sigma_t|t)` . Then `s_(t+1) | y_1, ..., y_t ~ hN(s_t+1|t, Sigma_t+1|t)` , where `s_t+1|t = A * s_t|t; Sigma_t+1|t = A * Sigma_t|t * A^T + Sigma_s` ( `Sigma_s` is the variance of `s`) <br>
    - Update step: compute `s_(t+1) | y_1, ..., y_(t+1)` <br>
    As we know above, we can prove that `s_(t+1) | y_1, ..., y_(t+1) ~ hN(s_t+1|t+1, Sigma_t+1|t+1)` , where `s_t+1|t+1 = s_t+1|t + K_t * (y_(t+1) - C * S_t+1|t); Sigma_t+1|t+1 = Sigma_t+1|t - K_t * C * Sigma_t+1|t` with `K_t := Sigma_t+1|t * C^T * (C * Sigma_t+1|t * C^T + Sigma_y)^-1` ( `Sigma_y` is the variance of `y` , and `K_t` is called the Kalman gain) <br>